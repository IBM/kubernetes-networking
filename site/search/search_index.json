{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes Networking \u00b6 In this Kubernetes Networking workshop, you will use different ways to control traffic on a Kubernetes cluster, from Service types, load balancing and Ingress, Network Policy and Calico, and VPC Gen2. Prerequirements \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Kubernetes cluster v1.18: for labs in Services , ClusterIP , and NodePort , you need a Kubernetes cluster with at least 1 worker node. for labs in LoadBalancer and Ingress you need a Kubernetes cluster with at least 2 worker nodes. for labs in Network Policy and Calico , you need a Kubernetes cluster with at least 2 worker nodes. for labs in VPC Gen2 , you need a Kubernetes cluster with at least 1 worker node. Go to Setup for more details. Labs \u00b6 Setup Services ClusterIP NodePort Loadbalancer NLB ExternalName Ingress ALB Network Policy and Calico VPC Gen2 Airgap","title":"About the workshop"},{"location":"#kubernetes-networking","text":"In this Kubernetes Networking workshop, you will use different ways to control traffic on a Kubernetes cluster, from Service types, load balancing and Ingress, Network Policy and Calico, and VPC Gen2.","title":"Kubernetes Networking"},{"location":"#prerequirements","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Kubernetes cluster v1.18: for labs in Services , ClusterIP , and NodePort , you need a Kubernetes cluster with at least 1 worker node. for labs in LoadBalancer and Ingress you need a Kubernetes cluster with at least 2 worker nodes. for labs in Network Policy and Calico , you need a Kubernetes cluster with at least 2 worker nodes. for labs in VPC Gen2 , you need a Kubernetes cluster with at least 1 worker node. Go to Setup for more details.","title":"Prerequirements"},{"location":"#labs","text":"Setup Services ClusterIP NodePort Loadbalancer NLB ExternalName Ingress ALB Network Policy and Calico VPC Gen2 Airgap","title":"Labs"},{"location":"airgap/","text":"Airgap \u00b6 An air gap , air wall , air gapping or disconnected network is a network security measure to ensure that a secure computer network is physically isolated from unsecured networks, such as the public Internet or an unsecured local area network. It means a computer or network has no network interfaces connected to other networks. For most enterprise clients in regulated industries and public sector agencies, the term air gap has a more specific meaning. Storage \u00b6 \"To move data between the outside world and the air-gapped system, it is necessary to write data to a physical medium such as a thumbdrive, and physically move it between computers.\" source Container Registry \u00b6 In an air-gapped Kubernetes cluster, you would not be able to deploy a Docker Hub image like ibmcom/guestbook:v1 . Instead, an air-gapped Kubernetes cluster would rely on a private registry. In theory, a container runtime can be configured with an image mirror to a private registry (thereby telling the container runtime to retrieve docker.io/ibmcom/guestbook:v1 from somewhere else, but it doesn't seem like this article is explaining that. Public Service Endpoint \u00b6 In a properly air-gapped Kubernetes cluster, you are not able to reach the control plane endpoint from outside the airgapped network, e.g. over the Internet. When you create an IBM Cloud Kubernetes Service with VPC instance, as in: ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID the created cluster will have public service endpoints. To disable the creation of these, use the --disable-public-service-endpoint flag. Best Practices \u00b6 Disable the public gateway for the VPC subnet with the worker nodes. This means that you cannot access the cluster nodes from outside the secure network, which includes for instance a build server and a container registry. Disable public endpoints for the control plane. When using private only endpoints, orgs can use systems within the VPC to manage the control plane, or set up Direct Link / VPN connections to their VPC. If this is not possible, proxying connections through a haproxy instance in the VPC is an alternative. How can a container engine on a worker in such a cluster pull container images to create containers? One option is to use a private registry inside the network. IBM Container Registry, the built-in OpenShift private registry, or a custom registry deployment inside the network are great options. Organizations can have their Continuous Integration (CI) systems tag and push images to IBM CR and then k8s/openshift clusters that don't have Internet access can pull images from IBM CR. Traffic to RESTful API endpoints on your cluster can be proxied through a secure gateway like API Connect with DataPower gateway.","title":"Airgap"},{"location":"airgap/#airgap","text":"An air gap , air wall , air gapping or disconnected network is a network security measure to ensure that a secure computer network is physically isolated from unsecured networks, such as the public Internet or an unsecured local area network. It means a computer or network has no network interfaces connected to other networks. For most enterprise clients in regulated industries and public sector agencies, the term air gap has a more specific meaning.","title":"Airgap"},{"location":"airgap/#storage","text":"\"To move data between the outside world and the air-gapped system, it is necessary to write data to a physical medium such as a thumbdrive, and physically move it between computers.\" source","title":"Storage"},{"location":"airgap/#container-registry","text":"In an air-gapped Kubernetes cluster, you would not be able to deploy a Docker Hub image like ibmcom/guestbook:v1 . Instead, an air-gapped Kubernetes cluster would rely on a private registry. In theory, a container runtime can be configured with an image mirror to a private registry (thereby telling the container runtime to retrieve docker.io/ibmcom/guestbook:v1 from somewhere else, but it doesn't seem like this article is explaining that.","title":"Container Registry"},{"location":"airgap/#public-service-endpoint","text":"In a properly air-gapped Kubernetes cluster, you are not able to reach the control plane endpoint from outside the airgapped network, e.g. over the Internet. When you create an IBM Cloud Kubernetes Service with VPC instance, as in: ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID the created cluster will have public service endpoints. To disable the creation of these, use the --disable-public-service-endpoint flag.","title":"Public Service Endpoint"},{"location":"airgap/#best-practices","text":"Disable the public gateway for the VPC subnet with the worker nodes. This means that you cannot access the cluster nodes from outside the secure network, which includes for instance a build server and a container registry. Disable public endpoints for the control plane. When using private only endpoints, orgs can use systems within the VPC to manage the control plane, or set up Direct Link / VPN connections to their VPC. If this is not possible, proxying connections through a haproxy instance in the VPC is an alternative. How can a container engine on a worker in such a cluster pull container images to create containers? One option is to use a private registry inside the network. IBM Container Registry, the built-in OpenShift private registry, or a custom registry deployment inside the network are great options. Organizations can have their Continuous Integration (CI) systems tag and push images to IBM CR and then k8s/openshift clusters that don't have Internet access can pull images from IBM CR. Traffic to RESTful API endpoints on your cluster can be proxied through a secure gateway like API Connect with DataPower gateway.","title":"Best Practices"},{"location":"clusterip/","text":"ClusterIP \u00b6 Add a Service to helloworld \u00b6 Now we have a basic understanding of the different ServiceTypes on Kubernetes, it is time to expose the Deployment of helloworld using a Service object. Create the Service object with the default type, MY_NS=my-apps kubectl create -f helloworld-service.yaml -n $MY_NS service/helloworld created Describe the Service, kubectl describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: <none> Selector: app=helloworld Type: ClusterIP IP: 172.21.49.18 Port: <unset> 8080/TCP TargetPort: http-server/TCP Endpoints: 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 Session Affinity: None Events: <none> You see that Kubernetes by default creates a Service of type ClusterIP . The service is now available and discoverable, but only within the cluster, using the Endpoints and port mapping found via the selector and labels . Get the endpoints that were created as part of the Service, kubectl get endpoints -n $MY_NS NAME ENDPOINTS AGE helloworld 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 42m You can define a Service without a Pod selector to abstract other kinds of backends than Pods. When a Service has no selector, the corresponding Endpoints object is not created automatically. You can manually map the Service to a network address and port by adding an Endpoints object manually, e.g. get and save the endpoint-helloworld.yaml to review the definition, kubectl get ep helloworld -n $MY_NS -o yaml > endpoint-helloworld.yaml or if vi is the default editor on your client, kubectl edit endpoints helloworld apiVersion: v1 kind: Endpoints metadata: name: helloworld namespace: default labels: app: helloworld subsets: - addresses: - ip: 172.30.153.79 targetRef: kind: Pod name: helloworld-5f8b6b587b-lwvcs The Endpoints object maps the Service object to a Pod on an internal IP address. Go to NodePort to learn more about ServiceType NodePort.","title":"ClusterIP"},{"location":"clusterip/#clusterip","text":"","title":"ClusterIP"},{"location":"clusterip/#add-a-service-to-helloworld","text":"Now we have a basic understanding of the different ServiceTypes on Kubernetes, it is time to expose the Deployment of helloworld using a Service object. Create the Service object with the default type, MY_NS=my-apps kubectl create -f helloworld-service.yaml -n $MY_NS service/helloworld created Describe the Service, kubectl describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: <none> Selector: app=helloworld Type: ClusterIP IP: 172.21.49.18 Port: <unset> 8080/TCP TargetPort: http-server/TCP Endpoints: 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 Session Affinity: None Events: <none> You see that Kubernetes by default creates a Service of type ClusterIP . The service is now available and discoverable, but only within the cluster, using the Endpoints and port mapping found via the selector and labels . Get the endpoints that were created as part of the Service, kubectl get endpoints -n $MY_NS NAME ENDPOINTS AGE helloworld 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 42m You can define a Service without a Pod selector to abstract other kinds of backends than Pods. When a Service has no selector, the corresponding Endpoints object is not created automatically. You can manually map the Service to a network address and port by adding an Endpoints object manually, e.g. get and save the endpoint-helloworld.yaml to review the definition, kubectl get ep helloworld -n $MY_NS -o yaml > endpoint-helloworld.yaml or if vi is the default editor on your client, kubectl edit endpoints helloworld apiVersion: v1 kind: Endpoints metadata: name: helloworld namespace: default labels: app: helloworld subsets: - addresses: - ip: 172.30.153.79 targetRef: kind: Pod name: helloworld-5f8b6b587b-lwvcs The Endpoints object maps the Service object to a Pod on an internal IP address. Go to NodePort to learn more about ServiceType NodePort.","title":"Add a Service to helloworld"},{"location":"externalname/","text":"External Name \u00b6 Pre-requisites \u00b6 Finish the Services , ClusterIP , NodePort , and LoadBalancer labs. External Name \u00b6 An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead, e.g. apiVersion: v1 kind: Service metadata: name: my-database-svc namespace: prod spec: type: ExternalName externalName: my.database.example.com When looking up the service my-database-svc.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record for my.database.example.com . Next, go to Ingress . Extra: Kubernetes Networking \u00b6 To learn more about Kubernetes Networking, go here .","title":"ExternalName"},{"location":"externalname/#external-name","text":"","title":"External Name"},{"location":"externalname/#pre-requisites","text":"Finish the Services , ClusterIP , NodePort , and LoadBalancer labs.","title":"Pre-requisites"},{"location":"externalname/#external-name_1","text":"An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead, e.g. apiVersion: v1 kind: Service metadata: name: my-database-svc namespace: prod spec: type: ExternalName externalName: my.database.example.com When looking up the service my-database-svc.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record for my.database.example.com . Next, go to Ingress .","title":"External Name"},{"location":"externalname/#extra-kubernetes-networking","text":"To learn more about Kubernetes Networking, go here .","title":"Extra: Kubernetes Networking"},{"location":"ingress-alb/","text":"Ingress and Application Load Balancer (ALB) \u00b6 Pre-requisites \u00b6 Finish the Services , ClusterIP , NodePort and LoadBalancer labs: Guestbook Deployment Guestbook Service of type LoadBalancer Logged in to IBM Cloud account Connected to Kubernetes cluster Network Administration \u00b6 When you create a standard cluster in IBM Cloud Kubernetes Service (IKS), a portable public subnet and a portable private subnet for the VLAN are automatically provisioned. You need account permissions to list the subnets, ibmcloud ks subnets --provider classic OK ID Network Gateway VLAN ID Type Bound Cluster Zone 2471290 10.176.98.248/29 10.176.98.249 3009946 private bvlntf2d0fe4l9hnres0 dal10 2078743 169.46.16.240/29 169.46.16.241 3009944 public bvlntf2d0fe4l9hnres0 dal10 or list the resources for the cluster, ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME Retrieving cluster remkohdev-iks118-1n-cluster1 and all its resources... OK Name: remkohdev-iks118-1n-cluster1 ID: bvlntf2d0fe4l9hnres0 State: normal Status: - Created: 2020-12-29T19:10:03+0000 Location: dal10 Pod Subnet: 172.30.0.0/16 Service Subnet: 172.21.0.0/16 Master URL: https://c111.us-south.containers.cloud.ibm.com:31666 Public Service Endpoint URL: https://c111.us-south.containers.cloud.ibm.com:31666 Private Service Endpoint URL: - Master Location: Dallas Master Status: Ready (1 day ago) Master State: deployed Master Health: normal Ingress Subdomain: remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000.us-south.containers.appdomain.cloud Ingress Secret: remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000 Ingress Status: healthy Ingress Message: All Ingress components are healthy Workers: 1 Worker Zones: dal10 Version: 1.18.13_1535 Creator: - Monitoring Dashboard: - Resource Group ID: 68af6383f717459686457a6434c4d19f Resource Group Name: Default Subnet VLANs VLAN ID Subnet CIDR Public User-managed 3009946 10.176.98.248/29 false false 3009944 169.46.16.240/29 true false The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public Network Load Balancer (NLB) services. To list all of the portable IP addresses in the IKS cluster, both used and available, you can retrieve the following ConfigMap in the kube-system namespace listing the resources of the subnets, kubectl get cm ibm-cloud-provider-vlan-ip-config -n kube-system -o yaml apiVersion: v1 data: cluster_id: bvlntf2d0fe4l9hnres0 reserved_private_ip: \"\" reserved_private_vlan_id: \"\" reserved_public_ip: \"\" reserved_public_vlan_id: \"\" vlanipmap.json: |- { \"vlans\": [ { \"id\": \"3009946\", \"subnets\": [ { \"id\": \"2471290\", \"ips\": [ \"10.176.98.250\", \"10.176.98.251\", \"10.176.98.252\", \"10.176.98.253\", \"10.176.98.254\" ], \"is_public\": false, \"is_byoip\": false, \"cidr\": \"10.176.98.248/29\" } ], \"zone\": \"dal10\", \"region\": \"us-south\" }, { \"id\": \"3009944\", \"subnets\": [ { \"id\": \"2078743\", \"ips\": [ \"169.46.16.242\", \"169.46.16.243\", \"169.46.16.244\", \"169.46.16.245\", \"169.46.16.246\" ], \"is_public\": true, \"is_byoip\": false, \"cidr\": \"169.46.16.240/29\" } ], \"zone\": \"dal10\", \"region\": \"us-south\" } ], \"vlan_errors\": [], \"reserved_ips\": [] } kind: ConfigMap metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: ibm-cloud-provider-vlan-ip-config namespace: kube-system selfLink: /api/v1/namespaces/kube-system/configmaps/ibm-cloud-provider-vlan-ip-config One of the public IP addresses on the public VLAN's subnet is assigned to the NLB. List the registered NLB host names and IP addresses in a cluster, ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000.us-south.containers.appdomain.cloud 169.46.16.242 None created remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000 default And retrieve the NodePort via, PORT=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PORT You see that the portable IP address 169.46.16.242 is assigned to the NLB. You can access the application via the portable IP address of the LoadBalancer NLB and service NodePort at http://169.46.16.242:$PORT. But LoadBalancer also has limitations. Ingress ALB \u00b6 Ingress is a reverse-proxy load balancer and Kubernetes API object that manages external access to services in a cluster. You can also use Ingress to expose multiple app services to a public or private network by using a single unique route. The Ingress API also supports TLS termination, virtual hosts, and path-based routing. Ingress consists of three components: Ingress resources Application load balancers (ALBs) A load balancer to handle incoming requests across zones. For classic clusters, this component is the multizone load balancer (MZLB) that IBM Cloud Kubernetes Service creates for you. For VPC clusters, this component is the VPC load balancer is created for you in your VPC. To expose an app with Ingress, you must create a Kubernetes service and register this with Ingress by defining an Ingress resource. One Ingress resource is required per namespace where you have apps that you want to expose. The Ingress resource is a Kubernetes resource that defines the rules for how to route incoming requests for apps. The Ingress resource also specifies the path to your app services. When you created a standard IKS cluster, an Ingress subdomain is already registered by default for your cluster. The paths to your app services are appended to the public route. In a standard cluster on IKS, the Ingress Application Load Balancer (ALB) is a layer 7 (L7) load balancer which implements the NGINX Ingress controller. A layer 4 (L4) LoadBalancer service exposes the ALB so that the ALB can receive external requests to your cluster. The ALB routes requests to app pods in your cluster based on distinguishing L7 protocol characteristics, such as HTTP request headers. Create an Ingress Resource for the HelloWorld App \u00b6 Instead of using <external-ip>:<nodeport> to access the HelloWorld app, I want to access our HelloWorld aplication via the URL <subdomain>/<path> . To access the app via the Ingress subdomain and a path rule, I define a path /hello in the Ingress resource. To configure your Ingress resource, you first also need the Ingress Subdomain and Ingress Secret of your cluster. Both were already created by IKS when you created the cluster. INGRESS_SUBDOMAIN=$(ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbHost') echo $INGRESS_SUBDOMAIN INGRESS_SECRET=$(ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbSslSecretName') echo $INGRESS_SECRET Or, INGRESS_SUBDOMAIN=$(ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressHostname') echo $INGRESS_SUBDOMAIN INGRESS_SECRET=$(ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressSecretName') echo $INGRESS_SECRET Create the Ingress resource using a rewrite path and change the hosts and host to the Ingress Subdomain of your cluster, and change the secretName to the value Ingress Secret of your cluster. echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress.yaml In version 1.19 syntax, apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \"public-iks-k8s-nginx\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / pathType: Prefix backend: service: name: helloworld port: number: 8080 The above resource will create an access path to helloworld at https://$INGRESS_SUBDOMAIN/hello. You can further customize Ingres routing with annotations to customize the ALB settings, TLS settings, request and response annocations, service limits, user authentication, or error actions. Make sure, the values for the hosts , secretName and host are set correctly to match the values of the Ingress Subdomain and Secret of your cluster. Edit the helloworld-ingress.yaml file to make the necessary changes, Then create the Ingress for helloworld, kubectl create -f helloworld-ingress.yaml -n $MY_NS ingress.networking.k8s.io/helloworld-ingress created Try to access the helloworld API and the proxy using the Ingress Subdomain with the path to the service, curl -L -X POST \"https://$INGRESS_SUBDOMAIN/hello/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world3\" }' {\"id\":\"40221ee9-06ac-4be2-97bc-2675c7cbb1e7\",\"sender\":\"world3\",\"message\":\"Hello world3 (direct)\",\"host\":null}% If you instead want to use subdomain paths instead of URI paths, echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: >- $INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080 - host: >- hello.$INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress-subdomain.yaml Delete the previous Ingress resource and create the Ingress resource using subdomain paths. kubectl get ingress -n $MY_NS kubectl delete ingress helloworld-ingress -n $MY_NS kubectl create -f helloworld-ingress-subdomain.yaml -n $MY_NS curl -L -X POST \"https://hello.$INGRESS_SUBDOMAIN/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world4\" }' {\"id\":\"6cb0489e-e290-4b98-9fd4-c4a303011898\",\"sender\":\"world4\",\"message\":\"Hello world4 (direct)\",\"host\":null} Next \u00b6 Next, go to Network Policy .","title":"Ingress and ALB"},{"location":"ingress-alb/#ingress-and-application-load-balancer-alb","text":"","title":"Ingress and Application Load Balancer (ALB)"},{"location":"ingress-alb/#pre-requisites","text":"Finish the Services , ClusterIP , NodePort and LoadBalancer labs: Guestbook Deployment Guestbook Service of type LoadBalancer Logged in to IBM Cloud account Connected to Kubernetes cluster","title":"Pre-requisites"},{"location":"ingress-alb/#network-administration","text":"When you create a standard cluster in IBM Cloud Kubernetes Service (IKS), a portable public subnet and a portable private subnet for the VLAN are automatically provisioned. You need account permissions to list the subnets, ibmcloud ks subnets --provider classic OK ID Network Gateway VLAN ID Type Bound Cluster Zone 2471290 10.176.98.248/29 10.176.98.249 3009946 private bvlntf2d0fe4l9hnres0 dal10 2078743 169.46.16.240/29 169.46.16.241 3009944 public bvlntf2d0fe4l9hnres0 dal10 or list the resources for the cluster, ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME Retrieving cluster remkohdev-iks118-1n-cluster1 and all its resources... OK Name: remkohdev-iks118-1n-cluster1 ID: bvlntf2d0fe4l9hnres0 State: normal Status: - Created: 2020-12-29T19:10:03+0000 Location: dal10 Pod Subnet: 172.30.0.0/16 Service Subnet: 172.21.0.0/16 Master URL: https://c111.us-south.containers.cloud.ibm.com:31666 Public Service Endpoint URL: https://c111.us-south.containers.cloud.ibm.com:31666 Private Service Endpoint URL: - Master Location: Dallas Master Status: Ready (1 day ago) Master State: deployed Master Health: normal Ingress Subdomain: remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000.us-south.containers.appdomain.cloud Ingress Secret: remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000 Ingress Status: healthy Ingress Message: All Ingress components are healthy Workers: 1 Worker Zones: dal10 Version: 1.18.13_1535 Creator: - Monitoring Dashboard: - Resource Group ID: 68af6383f717459686457a6434c4d19f Resource Group Name: Default Subnet VLANs VLAN ID Subnet CIDR Public User-managed 3009946 10.176.98.248/29 false false 3009944 169.46.16.240/29 true false The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public Network Load Balancer (NLB) services. To list all of the portable IP addresses in the IKS cluster, both used and available, you can retrieve the following ConfigMap in the kube-system namespace listing the resources of the subnets, kubectl get cm ibm-cloud-provider-vlan-ip-config -n kube-system -o yaml apiVersion: v1 data: cluster_id: bvlntf2d0fe4l9hnres0 reserved_private_ip: \"\" reserved_private_vlan_id: \"\" reserved_public_ip: \"\" reserved_public_vlan_id: \"\" vlanipmap.json: |- { \"vlans\": [ { \"id\": \"3009946\", \"subnets\": [ { \"id\": \"2471290\", \"ips\": [ \"10.176.98.250\", \"10.176.98.251\", \"10.176.98.252\", \"10.176.98.253\", \"10.176.98.254\" ], \"is_public\": false, \"is_byoip\": false, \"cidr\": \"10.176.98.248/29\" } ], \"zone\": \"dal10\", \"region\": \"us-south\" }, { \"id\": \"3009944\", \"subnets\": [ { \"id\": \"2078743\", \"ips\": [ \"169.46.16.242\", \"169.46.16.243\", \"169.46.16.244\", \"169.46.16.245\", \"169.46.16.246\" ], \"is_public\": true, \"is_byoip\": false, \"cidr\": \"169.46.16.240/29\" } ], \"zone\": \"dal10\", \"region\": \"us-south\" } ], \"vlan_errors\": [], \"reserved_ips\": [] } kind: ConfigMap metadata: labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: ibm-cloud-provider-vlan-ip-config namespace: kube-system selfLink: /api/v1/namespaces/kube-system/configmaps/ibm-cloud-provider-vlan-ip-config One of the public IP addresses on the public VLAN's subnet is assigned to the NLB. List the registered NLB host names and IP addresses in a cluster, ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000.us-south.containers.appdomain.cloud 169.46.16.242 None created remkohdev-iks118-1n-clu-47d7983a425e05fef831e694b7945b16-0000 default And retrieve the NodePort via, PORT=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PORT You see that the portable IP address 169.46.16.242 is assigned to the NLB. You can access the application via the portable IP address of the LoadBalancer NLB and service NodePort at http://169.46.16.242:$PORT. But LoadBalancer also has limitations.","title":"Network Administration"},{"location":"ingress-alb/#ingress-alb","text":"Ingress is a reverse-proxy load balancer and Kubernetes API object that manages external access to services in a cluster. You can also use Ingress to expose multiple app services to a public or private network by using a single unique route. The Ingress API also supports TLS termination, virtual hosts, and path-based routing. Ingress consists of three components: Ingress resources Application load balancers (ALBs) A load balancer to handle incoming requests across zones. For classic clusters, this component is the multizone load balancer (MZLB) that IBM Cloud Kubernetes Service creates for you. For VPC clusters, this component is the VPC load balancer is created for you in your VPC. To expose an app with Ingress, you must create a Kubernetes service and register this with Ingress by defining an Ingress resource. One Ingress resource is required per namespace where you have apps that you want to expose. The Ingress resource is a Kubernetes resource that defines the rules for how to route incoming requests for apps. The Ingress resource also specifies the path to your app services. When you created a standard IKS cluster, an Ingress subdomain is already registered by default for your cluster. The paths to your app services are appended to the public route. In a standard cluster on IKS, the Ingress Application Load Balancer (ALB) is a layer 7 (L7) load balancer which implements the NGINX Ingress controller. A layer 4 (L4) LoadBalancer service exposes the ALB so that the ALB can receive external requests to your cluster. The ALB routes requests to app pods in your cluster based on distinguishing L7 protocol characteristics, such as HTTP request headers.","title":"Ingress ALB"},{"location":"ingress-alb/#create-an-ingress-resource-for-the-helloworld-app","text":"Instead of using <external-ip>:<nodeport> to access the HelloWorld app, I want to access our HelloWorld aplication via the URL <subdomain>/<path> . To access the app via the Ingress subdomain and a path rule, I define a path /hello in the Ingress resource. To configure your Ingress resource, you first also need the Ingress Subdomain and Ingress Secret of your cluster. Both were already created by IKS when you created the cluster. INGRESS_SUBDOMAIN=$(ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbHost') echo $INGRESS_SUBDOMAIN INGRESS_SECRET=$(ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbSslSecretName') echo $INGRESS_SECRET Or, INGRESS_SUBDOMAIN=$(ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressHostname') echo $INGRESS_SUBDOMAIN INGRESS_SECRET=$(ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressSecretName') echo $INGRESS_SECRET Create the Ingress resource using a rewrite path and change the hosts and host to the Ingress Subdomain of your cluster, and change the secretName to the value Ingress Secret of your cluster. echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress.yaml In version 1.19 syntax, apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \"public-iks-k8s-nginx\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / pathType: Prefix backend: service: name: helloworld port: number: 8080 The above resource will create an access path to helloworld at https://$INGRESS_SUBDOMAIN/hello. You can further customize Ingres routing with annotations to customize the ALB settings, TLS settings, request and response annocations, service limits, user authentication, or error actions. Make sure, the values for the hosts , secretName and host are set correctly to match the values of the Ingress Subdomain and Secret of your cluster. Edit the helloworld-ingress.yaml file to make the necessary changes, Then create the Ingress for helloworld, kubectl create -f helloworld-ingress.yaml -n $MY_NS ingress.networking.k8s.io/helloworld-ingress created Try to access the helloworld API and the proxy using the Ingress Subdomain with the path to the service, curl -L -X POST \"https://$INGRESS_SUBDOMAIN/hello/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world3\" }' {\"id\":\"40221ee9-06ac-4be2-97bc-2675c7cbb1e7\",\"sender\":\"world3\",\"message\":\"Hello world3 (direct)\",\"host\":null}% If you instead want to use subdomain paths instead of URI paths, echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: >- $INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080 - host: >- hello.$INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress-subdomain.yaml Delete the previous Ingress resource and create the Ingress resource using subdomain paths. kubectl get ingress -n $MY_NS kubectl delete ingress helloworld-ingress -n $MY_NS kubectl create -f helloworld-ingress-subdomain.yaml -n $MY_NS curl -L -X POST \"https://hello.$INGRESS_SUBDOMAIN/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world4\" }' {\"id\":\"6cb0489e-e290-4b98-9fd4-c4a303011898\",\"sender\":\"world4\",\"message\":\"Hello world4 (direct)\",\"host\":null}","title":"Create an Ingress Resource for the HelloWorld App"},{"location":"ingress-alb/#next","text":"Next, go to Network Policy .","title":"Next"},{"location":"ingress-extra/","text":"Ingress Extra \u00b6 Allocation of an external IP address requires the system to create an external IP address, a forwarding rule, a target proxy, a backend service, and possibly an instance group. Once the IP address has been allocated you can connect to your service through it, assign it a domain name and distribute it to clients. Ingress Subdomain \u00b6 By default, IKS created an Ingress subdomain already when you created the cluster. You can also create a new Ingress subdomain or NLB hostname in addition, % kubectl config current-context % ibmcloud ks nlb-dns create classic -cluster remkohdev-iks116-3x-cluster --ip 169.48.75.82 OK NLB hostname was created as remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0001.us-south.containers.appdomain.cloud There is also the option to set up Direct Server Return (DSR) load balancing with an NLB 2.0 see here . When you create a Kubernetes LoadBalancer service for an app in your IKS cluster, a layer 7 Virtual Private Cloud (VPC) load balancer is automatically created in your VPC outside of your cluster. The VPC load balancer is multizonal and routes requests for your app through the private NodePorts that are automatically opened on your worker nodes. In a free cluster on IKS, the cluster's worker nodes are connected to an IBM-owned public VLAN and private VLAN by default. Because IBM controls the VLANs, subnets, and IP addresses, you cannot create multizone clusters or add subnets to your cluster, and can use only NodePort services to expose your app. If your cluster is a free cluster or only has 1 worker node, the EXTERNAL-IP will say pending because in vain the cluster is waiting on the asynchronous response. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public network load balancer services, or NLBs. The portable private subnet provides 5 usable IP addresses. 1 portable private IP address is used by the default private Ingress ALB. The remaining 4 portable private IP addresses can be used to expose single apps to a private network by creating private load balancer services, or NLBs. Resources \u00b6 Components and architecture of an NLB 1.0 . Choosing an app exposure service, choosing a deployment pattern for classic clusters . Quick start for load balancers Classic: Setting up DSR load balancing with an NLB 2.0 (beta)","title":"Ingress Extra"},{"location":"ingress-extra/#ingress-extra","text":"Allocation of an external IP address requires the system to create an external IP address, a forwarding rule, a target proxy, a backend service, and possibly an instance group. Once the IP address has been allocated you can connect to your service through it, assign it a domain name and distribute it to clients.","title":"Ingress Extra"},{"location":"ingress-extra/#ingress-subdomain","text":"By default, IKS created an Ingress subdomain already when you created the cluster. You can also create a new Ingress subdomain or NLB hostname in addition, % kubectl config current-context % ibmcloud ks nlb-dns create classic -cluster remkohdev-iks116-3x-cluster --ip 169.48.75.82 OK NLB hostname was created as remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0001.us-south.containers.appdomain.cloud There is also the option to set up Direct Server Return (DSR) load balancing with an NLB 2.0 see here . When you create a Kubernetes LoadBalancer service for an app in your IKS cluster, a layer 7 Virtual Private Cloud (VPC) load balancer is automatically created in your VPC outside of your cluster. The VPC load balancer is multizonal and routes requests for your app through the private NodePorts that are automatically opened on your worker nodes. In a free cluster on IKS, the cluster's worker nodes are connected to an IBM-owned public VLAN and private VLAN by default. Because IBM controls the VLANs, subnets, and IP addresses, you cannot create multizone clusters or add subnets to your cluster, and can use only NodePort services to expose your app. If your cluster is a free cluster or only has 1 worker node, the EXTERNAL-IP will say pending because in vain the cluster is waiting on the asynchronous response. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public network load balancer services, or NLBs. The portable private subnet provides 5 usable IP addresses. 1 portable private IP address is used by the default private Ingress ALB. The remaining 4 portable private IP addresses can be used to expose single apps to a private network by creating private load balancer services, or NLBs.","title":"Ingress Subdomain"},{"location":"ingress-extra/#resources","text":"Components and architecture of an NLB 1.0 . Choosing an app exposure service, choosing a deployment pattern for classic clusters . Quick start for load balancers Classic: Setting up DSR load balancing with an NLB 2.0 (beta)","title":"Resources"},{"location":"loadbalancer/","text":"Loadbalancer and Network Load Balancer (NLB) 1.0 \u00b6 Pre-requisites \u00b6 Finish the Services , ClusterIP , and NodePort labs. LoadBalancer \u00b6 In the previous labs, you created a service for the helloworld application with a default clusterIP and then added a NodePort to the Service, which proxies requests to the Service resource. But in a production environment, you still need a load balancer, whether client requests are internal or external coming in over the public network. The LoadBalancer service in Kubernetes configures an L4 TCP Load Balancer that forwards and balances traffic from the internet to your backend application. To use a load balancer for distributing client traffic to the nodes in a cluster, you need a public IP address that the clients can connect to, and you need IP addresses on the nodes themselves to which the load balancer can forward the requests. Services of type LoadBalancer have some limitations. It cannot do TLS termination, do virtual hosts or path-based routing, so you can\u2019t use a single load balancer to proxy to multiple services. These limitations led to the addition in Kubernetes v1.2 of a separate kubernetes resource called Ingress . The actual creation of the load balancer happens asynchronously with the Service, so you might have to wait until the load balancer has been created. Load Balancer on IKS \u00b6 The LoadBalancer service type is implemented differently depending on your cluster's infrastructure provider. On IKS (IBM Kubernetes Service), classic clusters implements a Network Load Balancer (NLB) 1.0 by default. Version 1.0 NLBs use network address translation (NAT) to rewrite the request packet's source IP address to the IP of worker node where a load balancer pod exists. Version 2.0 NLBs don't use NAT but IP over IP (IPIP) to encapsulate the original request packet into another packet, which preserves the client IP as its source IP address. The worker node then uses direct server return (DSR) to send the app response packet to the client IP. When you create a standard cluster, IKS automatically provisions a portable public subnet and a portable private subnet. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB . The remaining 4 portable public IP addresses can be used to expose single apps using layer 4 (L4) TCP/UDP Network Load Balancer (NLB). The portable public and private IP addresses are static floating IPs pointing to worker nodes. A Keepalived daemon constantly monitors the IP, and automatically moves the IP to another worker node if the worker node is removed. See: Classic: About network load balancers (NLBs) . Load Balancing Methods on IKS \u00b6 Before we create a load balancer with NLB v1.0 + subdomain for the helloworld application, review the different Load Balancing Methods on IKS: NodePort exposes the app via a port and public IP address on a worker node using kube-proxy . LoadBalancer NLB v1.0 + subdomain uses basic load balancing that exposes the app with an IP address or a subdomain. LoadBalancer NLB v2.0 + subdomain , uses Direct Server Return (DSR) load balancing, which does not change the packets but the destination address, and exposes the app with an IP address or a subdomain, supports SSL termination. Network load balancer (NLB) 2.0 is in beta. To [specify a load balancer 2.0'(https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer-v2#ipvs_single_zone_config) you must add annotation service.kubernetes.io/ibm-load-balancer-cloud-provider-enable-features: \"ipvs\" to the service. Istio Ingress Gateway + NLB uses basic load balancing that exposes the app with a subdomain and uses Istio routing rules. Ingress with public ALB uses HTTPS load balancing that exposes the app with a subdomain and uses custom routing rules and SSL termination for multiple apps. Customize the ALB routing rules with annotations . Custom Ingress + NLB uses HTTPS load balancing with a custom Ingress that exposes the app with the IBM-provided ALB subdomain and uses custom routing rules. Create a Network Load Balancer v1.0 \u00b6 In the previous lab, you already created a NodePort Service. kubectl get svc -n $MY_NS Patch the service for helloworld and change the type to LoadBalancer . kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' -n $MY_NS The TYPE should be set to LoadBalancer now, and an EXTERNAL-IP should be assigned. kubectl get svc helloworld -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 172.21.161.255 169.48.67.163 8080:31777/TCP 24m Describe the helloworld LoadBalancer Service, kubectl describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: <none> Selector: app=helloworld Type: LoadBalancer IP: 172.21.49.18 LoadBalancer Ingress: 169.46.16.244 Port: <unset> 8080/TCP TargetPort: http-server/TCP NodePort: <unset> 30785/TCP Endpoints: 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> Now to access the Service of the helloworld from the public internet, you can use the public IP address of the NLB and the assigned NodePort of the service in the format <IP_address>:<port> . NodePorts are accessible on every public and private IP address of every worker node within the cluster. PUBLIC_IP=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip') echo $PUBLIC_IP PORT=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PORT Access the helloworld app in a browser or with Curl, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world2\" }' {\"id\":\"73987562-8844-43b9-80a0-49aad2d413e9\",\"sender\":\"world2\",\"message\":\"Hello remko (direct)\",\"host\":null} Next, go to ExternalName .","title":"Loadbalancer and NLB"},{"location":"loadbalancer/#loadbalancer-and-network-load-balancer-nlb-10","text":"","title":"Loadbalancer and Network Load Balancer (NLB) 1.0"},{"location":"loadbalancer/#pre-requisites","text":"Finish the Services , ClusterIP , and NodePort labs.","title":"Pre-requisites"},{"location":"loadbalancer/#loadbalancer","text":"In the previous labs, you created a service for the helloworld application with a default clusterIP and then added a NodePort to the Service, which proxies requests to the Service resource. But in a production environment, you still need a load balancer, whether client requests are internal or external coming in over the public network. The LoadBalancer service in Kubernetes configures an L4 TCP Load Balancer that forwards and balances traffic from the internet to your backend application. To use a load balancer for distributing client traffic to the nodes in a cluster, you need a public IP address that the clients can connect to, and you need IP addresses on the nodes themselves to which the load balancer can forward the requests. Services of type LoadBalancer have some limitations. It cannot do TLS termination, do virtual hosts or path-based routing, so you can\u2019t use a single load balancer to proxy to multiple services. These limitations led to the addition in Kubernetes v1.2 of a separate kubernetes resource called Ingress . The actual creation of the load balancer happens asynchronously with the Service, so you might have to wait until the load balancer has been created.","title":"LoadBalancer"},{"location":"loadbalancer/#load-balancer-on-iks","text":"The LoadBalancer service type is implemented differently depending on your cluster's infrastructure provider. On IKS (IBM Kubernetes Service), classic clusters implements a Network Load Balancer (NLB) 1.0 by default. Version 1.0 NLBs use network address translation (NAT) to rewrite the request packet's source IP address to the IP of worker node where a load balancer pod exists. Version 2.0 NLBs don't use NAT but IP over IP (IPIP) to encapsulate the original request packet into another packet, which preserves the client IP as its source IP address. The worker node then uses direct server return (DSR) to send the app response packet to the client IP. When you create a standard cluster, IKS automatically provisions a portable public subnet and a portable private subnet. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB . The remaining 4 portable public IP addresses can be used to expose single apps using layer 4 (L4) TCP/UDP Network Load Balancer (NLB). The portable public and private IP addresses are static floating IPs pointing to worker nodes. A Keepalived daemon constantly monitors the IP, and automatically moves the IP to another worker node if the worker node is removed. See: Classic: About network load balancers (NLBs) .","title":"Load Balancer on IKS"},{"location":"loadbalancer/#load-balancing-methods-on-iks","text":"Before we create a load balancer with NLB v1.0 + subdomain for the helloworld application, review the different Load Balancing Methods on IKS: NodePort exposes the app via a port and public IP address on a worker node using kube-proxy . LoadBalancer NLB v1.0 + subdomain uses basic load balancing that exposes the app with an IP address or a subdomain. LoadBalancer NLB v2.0 + subdomain , uses Direct Server Return (DSR) load balancing, which does not change the packets but the destination address, and exposes the app with an IP address or a subdomain, supports SSL termination. Network load balancer (NLB) 2.0 is in beta. To [specify a load balancer 2.0'(https://cloud.ibm.com/docs/containers?topic=containers-loadbalancer-v2#ipvs_single_zone_config) you must add annotation service.kubernetes.io/ibm-load-balancer-cloud-provider-enable-features: \"ipvs\" to the service. Istio Ingress Gateway + NLB uses basic load balancing that exposes the app with a subdomain and uses Istio routing rules. Ingress with public ALB uses HTTPS load balancing that exposes the app with a subdomain and uses custom routing rules and SSL termination for multiple apps. Customize the ALB routing rules with annotations . Custom Ingress + NLB uses HTTPS load balancing with a custom Ingress that exposes the app with the IBM-provided ALB subdomain and uses custom routing rules.","title":"Load Balancing Methods on IKS"},{"location":"loadbalancer/#create-a-network-load-balancer-v10","text":"In the previous lab, you already created a NodePort Service. kubectl get svc -n $MY_NS Patch the service for helloworld and change the type to LoadBalancer . kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' -n $MY_NS The TYPE should be set to LoadBalancer now, and an EXTERNAL-IP should be assigned. kubectl get svc helloworld -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 172.21.161.255 169.48.67.163 8080:31777/TCP 24m Describe the helloworld LoadBalancer Service, kubectl describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: <none> Selector: app=helloworld Type: LoadBalancer IP: 172.21.49.18 LoadBalancer Ingress: 169.46.16.244 Port: <unset> 8080/TCP TargetPort: http-server/TCP NodePort: <unset> 30785/TCP Endpoints: 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> Now to access the Service of the helloworld from the public internet, you can use the public IP address of the NLB and the assigned NodePort of the service in the format <IP_address>:<port> . NodePorts are accessible on every public and private IP address of every worker node within the cluster. PUBLIC_IP=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip') echo $PUBLIC_IP PORT=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PORT Access the helloworld app in a browser or with Curl, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world2\" }' {\"id\":\"73987562-8844-43b9-80a0-49aad2d413e9\",\"sender\":\"world2\",\"message\":\"Hello remko (direct)\",\"host\":null} Next, go to ExternalName .","title":"Create a Network Load Balancer v1.0"},{"location":"networking-extra/","text":"Kubernetes Networking Extra \u00b6 Kubernetes Networking in-depth \u00b6 The following image demonstrates how Kubernetes forwards public network traffic through kube-proxy and NodePort, LoadBalancer, or Ingress services in IBM Cloud Kubernetes Service. The following image shows what features each of the different ServiceTypes supports on IBM Cloud. The services network is implemented by a kubernetes component called kube-proxy collaborating with a linux kernel module called netfilter to trap and reroute traffic sent to the cluster IP so that it is sent to a healthy pod instead Connections and requests operate at OSI layer 4 (tcp) or layer 7 (http, rpc, etc). Netfilter rules are routing rules, and they operate on IP packets at layer 3. All routers, including netfilter, make routing decisions based more or less solely on information contained in the packet; generally where it is from and where it is going. So to describe this behavior in layer 3 terms: each packet destined for the service at10.3.241.152:80 that arrives at a node\u2019s eth0 interface is processed by netfilter, matches the rules established for our service, and is forwarded to the IP of a healthy pod. External clients that call into our pods has to make use of this same routing infrastructure. The cluster IP and port is the \u201cfront end\u201d. The cluster IP of a service is only reachable from a node\u2019s ethernet interface. How can we forward traffic from a publicly visible IP endpoint to an IP that is only reachable once the packet is already on a node? One way is to examine the netfilter rules using the iptables utility. Any packet from anywhere that arrives on the node\u2019s ethernet interface with a destination of 10.3.241.152:80 is going to match and get routed to a pod. So you could give clients the cluster IP or a friendly domain name, and then add a route to get the packets to one of the nodes. But routers operating on layer 3 packets don\u2019t know healthy services from unhealthy. Kube-proxy\u2019s role is to actively manage netfilter. We can\u2019t easily create a stable static route between the gateway router and the nodes using the service network (cluster IP). Kube-Proxy \u00b6 Kubernetes relies on proxying to forward inbound traffic to backends. When you use the name of the service, it looks up the name in the cluster DNS provider and routes the request to the in-cluster IP address of the service. A Kubernetes cluster runs a local Kubernetes network proxy, kube-proxy , as a daemon on each worker node in the kube-system namespace, which provides basic load balancing for services. The default load balancing mode in Kubernetes is iptables and rules, a Linux kernel feature, to direct requests to the pods behind a service equally. The native method for load distribution in iptables mode is random selection . An older kube-proxy mode is userspace , which uses round-robin load distribution, allocating the next available pod on an IP list, then rotating the list. Proxy modes: - userspace - iptables (default) (NLB 1.0 uses iptables) - IPVS (NLB 2.0 uses IP Virtual Server (IPVS)) DNS \u00b6 A cluster-aware DNS server, such as CoreDNS , watches the Kubernetes API for new Services and creates a set of DNS records for each service. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name. Kubernetes also supports DNS SRV (DNS Service) records for named ports. If the \"my-service.my-ns\" Service has a port named \"http\" with the protocol set to TCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for \"http\", as well as the IP address. The Kubernetes DNS server is the only way to access ExternalName Services. Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm. To verify if the CoreDNS deployment is available, kubectl get deployment -n kube-system | grep dns coredns 3/3 3 3 7h51m coredns-autoscaler 1/1 1 1 7h51m kubectl get configmap -n kube-system | grep dns coredns 1 11d coredns-autoscaler 1 11d node-local-dns 1 11d The node-local-dns is the NodeLocal DNSCaching agent for improved cluster DNS performance. NodeLocal DNSCache is a feature introduced as stable in v1.18. Next, go back to continue. Resources \u00b6 Choosing an app exposure service ExternalName Service supported protocols","title":"Kubernetes Networking Extra"},{"location":"networking-extra/#kubernetes-networking-extra","text":"","title":"Kubernetes Networking Extra"},{"location":"networking-extra/#kubernetes-networking-in-depth","text":"The following image demonstrates how Kubernetes forwards public network traffic through kube-proxy and NodePort, LoadBalancer, or Ingress services in IBM Cloud Kubernetes Service. The following image shows what features each of the different ServiceTypes supports on IBM Cloud. The services network is implemented by a kubernetes component called kube-proxy collaborating with a linux kernel module called netfilter to trap and reroute traffic sent to the cluster IP so that it is sent to a healthy pod instead Connections and requests operate at OSI layer 4 (tcp) or layer 7 (http, rpc, etc). Netfilter rules are routing rules, and they operate on IP packets at layer 3. All routers, including netfilter, make routing decisions based more or less solely on information contained in the packet; generally where it is from and where it is going. So to describe this behavior in layer 3 terms: each packet destined for the service at10.3.241.152:80 that arrives at a node\u2019s eth0 interface is processed by netfilter, matches the rules established for our service, and is forwarded to the IP of a healthy pod. External clients that call into our pods has to make use of this same routing infrastructure. The cluster IP and port is the \u201cfront end\u201d. The cluster IP of a service is only reachable from a node\u2019s ethernet interface. How can we forward traffic from a publicly visible IP endpoint to an IP that is only reachable once the packet is already on a node? One way is to examine the netfilter rules using the iptables utility. Any packet from anywhere that arrives on the node\u2019s ethernet interface with a destination of 10.3.241.152:80 is going to match and get routed to a pod. So you could give clients the cluster IP or a friendly domain name, and then add a route to get the packets to one of the nodes. But routers operating on layer 3 packets don\u2019t know healthy services from unhealthy. Kube-proxy\u2019s role is to actively manage netfilter. We can\u2019t easily create a stable static route between the gateway router and the nodes using the service network (cluster IP).","title":"Kubernetes Networking in-depth"},{"location":"networking-extra/#kube-proxy","text":"Kubernetes relies on proxying to forward inbound traffic to backends. When you use the name of the service, it looks up the name in the cluster DNS provider and routes the request to the in-cluster IP address of the service. A Kubernetes cluster runs a local Kubernetes network proxy, kube-proxy , as a daemon on each worker node in the kube-system namespace, which provides basic load balancing for services. The default load balancing mode in Kubernetes is iptables and rules, a Linux kernel feature, to direct requests to the pods behind a service equally. The native method for load distribution in iptables mode is random selection . An older kube-proxy mode is userspace , which uses round-robin load distribution, allocating the next available pod on an IP list, then rotating the list. Proxy modes: - userspace - iptables (default) (NLB 1.0 uses iptables) - IPVS (NLB 2.0 uses IP Virtual Server (IPVS))","title":"Kube-Proxy"},{"location":"networking-extra/#dns","text":"A cluster-aware DNS server, such as CoreDNS , watches the Kubernetes API for new Services and creates a set of DNS records for each service. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name. Kubernetes also supports DNS SRV (DNS Service) records for named ports. If the \"my-service.my-ns\" Service has a port named \"http\" with the protocol set to TCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for \"http\", as well as the IP address. The Kubernetes DNS server is the only way to access ExternalName Services. Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm. To verify if the CoreDNS deployment is available, kubectl get deployment -n kube-system | grep dns coredns 3/3 3 3 7h51m coredns-autoscaler 1/1 1 1 7h51m kubectl get configmap -n kube-system | grep dns coredns 1 11d coredns-autoscaler 1 11d node-local-dns 1 11d The node-local-dns is the NodeLocal DNSCaching agent for improved cluster DNS performance. NodeLocal DNSCache is a feature introduced as stable in v1.18. Next, go back to continue.","title":"DNS"},{"location":"networking-extra/#resources","text":"Choosing an app exposure service ExternalName Service supported protocols","title":"Resources"},{"location":"networkpolicy/","text":"Network Policy and Calico \u00b6 Prerequirements \u00b6 Finish the Services , ClusterIP , NodePort , LoadBalancer , and Ingress labs, Guestbook Deployment Guestbook Service of type LoadBalancer Logged in to IBM Cloud account Connected to Kubernetes cluster Network Policy and Calico \u00b6 To control traffic flow at the IP address or port level (OSI layer 3 or 4), you can use Kubernetes NetworkPolicies. Network policies are implemented by a Network Plugin. When defining a pod- or namespace- based NetworkPolicy, labels are used to select pods and define rules which specify what traffic is allowed to the selected pods. For IP based NetworkPolicies, you define policies based on IP blocks (CIDR ranges). By default, pods are non-isolated and accept traffic from any source. Once there is any NetworkPolicy in a namespace selecting a particular pod, that pod will be isolated and reject any connections not allowed by any NetworkPolicy. Network policies do not conflict; they are additive. A pod is restricted to what is allowed by the union of policies\u2019 ingress/egress rules. Thus, order of evaluation does not affect the policy result. There are four kinds of selectors in an ingress from section or egress to section: podSelector, namespaceSelector, podSelector and namespaceSelector, ipBlock for IP CIDR ranges. The following example allows traffic from a frontend application to a backend application, apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: my-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - podSelector: matchLabels: role: backend ports: - protocol: TCP port: 5978 The following example denies all ingress traffic, apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress Every IBM Cloud Kubernetes Service cluster is set up with a network plug-in called Calico. Default network policies are set up to secure the public network interface of every worker node in the cluster. You can use Kubernetes and Calico to create network policies for a cluster. With Kubernetes network policies, you can specify the network traffic that you want to allow or block to and from a pod within a cluster. To set more advanced network policies such as blocking inbound (ingress) traffic to a Network Load Balancer (NLB) services, use Calico network policies. When a Kubernetes network policy is applied, it is automatically converted into a Calico network policy so that Calico can apply it as an Iptables rule. Both incoming and outgoing network traffic can be allowed or blocked based on protocol, port, and source or destination IP addresses. Traffic can also be filtered based on pod and namespace labels. Calico network policies are a superset of the Kubernetes network policies and are applied by using calicoctl commands. Calico policies add the following features: - Allow or block network traffic on specific network interfaces regardless of the Kubernetes pod source or destination IP address or CIDR. - Allow or block network traffic for pods across namespaces. - Block inbound traffic to Kubernetes LoadBalancer or NodePort services. Calico enforces these policies, including any Kubernetes network policies that are automatically converted to Calico policies, by setting up Linux Iptables rules on the Kubernetes worker nodes. Iptables rules serve as a firewall for the worker node to define the characteristics that the network traffic must meet to be forwarded to the targeted resource. Calico network policies and Calico global network policies are applied using calicoctl. Syntax is similar to Kubernetes, but there a few differences Zero Trust Network Model \u00b6 Adopting a zero trust network model is best practice for securing workloads and hosts in your cloud-native strategy. Create helloworld Proxy \u00b6 For this tutorial, we will use an additional app called helloworld-proxy , which proxies requests to the helloworld app. Deploy the helloworld proxy to the same namespace as the helloworld app, kubectl create -f helloworld-proxy-deployment.yaml -n $MY_NS kubectl create -f helloworld-proxy-service-loadbalancer.yaml -n $MY_NS The deployment in your project namespace should now look as follows, kubectl get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-77bc887769-7n8wg 1/1 Running 0 27h pod/helloworld-77bc887769-cts4v 1/1 Running 0 27h pod/helloworld-77bc887769-w4tmc 1/1 Running 0 27h pod/helloworld-proxy-5f689cf785-k7n6v 1/1 Running 0 15s pod/helloworld-proxy-5f689cf785-qmmlz 1/1 Running 0 15s pod/helloworld-proxy-5f689cf785-wh7x7 1/1 Running 0 15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/helloworld LoadBalancer 172.21.145.243 169.61.252.3 8080:30663/TCP 27h service/helloworld-proxy LoadBalancer 172.21.74.243 169.61.252.4 8080:30005/TCP 14s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3/3 3 3 27h deployment.apps/helloworld-proxy 3/3 3 3 15s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-77bc887769 3 3 3 27h replicaset.apps/helloworld-proxy-5f689cf785 3 3 3 15s Get the proxy service details and test the proxy, PROXY_HOST=$(kubectl get svc helloworld-proxy -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip') echo $PROXY_HOST PROXY_NODEPORT=$(kubectl get svc helloworld-proxy -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort') echo $PROXY_NODEPORT Test the helloworld-proxy app, add the host: helloworld:8080 property in the data object, which tells the helloworld-proxy app to proxy the message to the host app, and send the request to the /proxy/api/messages endpoint of our helloworld-proxy app, curl -L -X POST \"http://$PROXY_HOST:$PROXY_NODEPORT/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' {\"id\":\"6e5ef78f-9e09-42f8-8294-347ece7cd07f\",\"sender\":\"remko\",\"message\":\"Hello remko (proxy)\",\"host\":\"helloworld:8080\"} Apply Network Policy - Allow No Traffic \u00b6 Define the Network Policy file to deny all traffic, echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: helloworld-deny-all spec: podSelector: {} policyTypes: - Ingress - Egress' > helloworld-policy-denyall.yaml Create the Network Policy, kubectl create -f helloworld-policy-denyall.yaml -n $MY_NS networkpolicy.networking.k8s.io/helloworld-deny-all created Test both the helloworld and the helloworld-proxy apps, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' curl: (7) Failed to connect to 52.118.3.110 port 30663: Operation timed out curl -L -X POST \"http://$PROXY_HOST:$PROXY_PORT/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: (7) Failed to connect to 169.61.252.4 port 80: Operation timed out It takes quite a long time before connections time out. All traffic is denied, despite that we have a LoadBalancer service added to each deployment, kubectl get svc -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 172.21.145.243 169.61.252.3 8080:30663/TCP 30h helloworld-proxy LoadBalancer 172.21.74.243 169.61.252.4 8080:30005/TCP 141m Apply Network Policy - Allow Only Traffic From Proxy \u00b6 Let's allow direct ingress traffic to the helloworld app on port 8080 , but not allow traffic to the helloworld-proxy app. Define the Network Policy file, echo 'apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: name: helloworld-allow spec: selector: app == 'helloworld' types: - Ingress ingress: - action: Allow protocol: TCP source: selector: role == 'helloworld-proxy' destination: ports: - 8080' > helloworld-calico-allow.yaml Create the Network Policy, calicoctl create -f helloworld-calico-allow.yaml Successfully created 1 'GlobalNetworkPolicy' resource(s) Review the existing NetworkPolices in the project namespace, kubectl get networkpolicies -n $MY_NS NAME POD-SELECTOR AGE helloworld-deny-all <none> 3h10m Review the existing GlobalNetworkPolicies, kubectl get globalnetworkpolicies NAME AGE default.allow-all-outbound 33h default.allow-all-private-default 33h default.allow-bigfix-port 33h default.allow-icmp 33h default.allow-node-port-dnat 33h default.allow-sys-mgmt 33h default.allow-vrrp 33h default.helloworld-allow 14ms Test the helloworld and the `helloworld-proxy' apps again, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' {\"id\":\"19b0a0c0-ef05-4c91-8dd1-330e40d252ef\",\"sender\":\"remko\",\"message\":\"Hello remko (direct)\",\"host\":null} curl -L -X POST \"http://$PROXY_HOST:$PROXY_PORT/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: (7) Failed to connect to 169.61.252.4 port 80: Operation timed out Cleanup \u00b6 kubectl delete globalnetworkpolicy default.helloworld-allow kubectl delete networkpolicy helloworld-deny-all","title":"Network Policy and Calico"},{"location":"networkpolicy/#network-policy-and-calico","text":"","title":"Network Policy and Calico"},{"location":"networkpolicy/#prerequirements","text":"Finish the Services , ClusterIP , NodePort , LoadBalancer , and Ingress labs, Guestbook Deployment Guestbook Service of type LoadBalancer Logged in to IBM Cloud account Connected to Kubernetes cluster","title":"Prerequirements"},{"location":"networkpolicy/#network-policy-and-calico_1","text":"To control traffic flow at the IP address or port level (OSI layer 3 or 4), you can use Kubernetes NetworkPolicies. Network policies are implemented by a Network Plugin. When defining a pod- or namespace- based NetworkPolicy, labels are used to select pods and define rules which specify what traffic is allowed to the selected pods. For IP based NetworkPolicies, you define policies based on IP blocks (CIDR ranges). By default, pods are non-isolated and accept traffic from any source. Once there is any NetworkPolicy in a namespace selecting a particular pod, that pod will be isolated and reject any connections not allowed by any NetworkPolicy. Network policies do not conflict; they are additive. A pod is restricted to what is allowed by the union of policies\u2019 ingress/egress rules. Thus, order of evaluation does not affect the policy result. There are four kinds of selectors in an ingress from section or egress to section: podSelector, namespaceSelector, podSelector and namespaceSelector, ipBlock for IP CIDR ranges. The following example allows traffic from a frontend application to a backend application, apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: my-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - podSelector: matchLabels: role: backend ports: - protocol: TCP port: 5978 The following example denies all ingress traffic, apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress Every IBM Cloud Kubernetes Service cluster is set up with a network plug-in called Calico. Default network policies are set up to secure the public network interface of every worker node in the cluster. You can use Kubernetes and Calico to create network policies for a cluster. With Kubernetes network policies, you can specify the network traffic that you want to allow or block to and from a pod within a cluster. To set more advanced network policies such as blocking inbound (ingress) traffic to a Network Load Balancer (NLB) services, use Calico network policies. When a Kubernetes network policy is applied, it is automatically converted into a Calico network policy so that Calico can apply it as an Iptables rule. Both incoming and outgoing network traffic can be allowed or blocked based on protocol, port, and source or destination IP addresses. Traffic can also be filtered based on pod and namespace labels. Calico network policies are a superset of the Kubernetes network policies and are applied by using calicoctl commands. Calico policies add the following features: - Allow or block network traffic on specific network interfaces regardless of the Kubernetes pod source or destination IP address or CIDR. - Allow or block network traffic for pods across namespaces. - Block inbound traffic to Kubernetes LoadBalancer or NodePort services. Calico enforces these policies, including any Kubernetes network policies that are automatically converted to Calico policies, by setting up Linux Iptables rules on the Kubernetes worker nodes. Iptables rules serve as a firewall for the worker node to define the characteristics that the network traffic must meet to be forwarded to the targeted resource. Calico network policies and Calico global network policies are applied using calicoctl. Syntax is similar to Kubernetes, but there a few differences","title":"Network Policy and Calico"},{"location":"networkpolicy/#zero-trust-network-model","text":"Adopting a zero trust network model is best practice for securing workloads and hosts in your cloud-native strategy.","title":"Zero Trust Network Model"},{"location":"networkpolicy/#create-helloworld-proxy","text":"For this tutorial, we will use an additional app called helloworld-proxy , which proxies requests to the helloworld app. Deploy the helloworld proxy to the same namespace as the helloworld app, kubectl create -f helloworld-proxy-deployment.yaml -n $MY_NS kubectl create -f helloworld-proxy-service-loadbalancer.yaml -n $MY_NS The deployment in your project namespace should now look as follows, kubectl get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-77bc887769-7n8wg 1/1 Running 0 27h pod/helloworld-77bc887769-cts4v 1/1 Running 0 27h pod/helloworld-77bc887769-w4tmc 1/1 Running 0 27h pod/helloworld-proxy-5f689cf785-k7n6v 1/1 Running 0 15s pod/helloworld-proxy-5f689cf785-qmmlz 1/1 Running 0 15s pod/helloworld-proxy-5f689cf785-wh7x7 1/1 Running 0 15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/helloworld LoadBalancer 172.21.145.243 169.61.252.3 8080:30663/TCP 27h service/helloworld-proxy LoadBalancer 172.21.74.243 169.61.252.4 8080:30005/TCP 14s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3/3 3 3 27h deployment.apps/helloworld-proxy 3/3 3 3 15s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-77bc887769 3 3 3 27h replicaset.apps/helloworld-proxy-5f689cf785 3 3 3 15s Get the proxy service details and test the proxy, PROXY_HOST=$(kubectl get svc helloworld-proxy -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip') echo $PROXY_HOST PROXY_NODEPORT=$(kubectl get svc helloworld-proxy -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort') echo $PROXY_NODEPORT Test the helloworld-proxy app, add the host: helloworld:8080 property in the data object, which tells the helloworld-proxy app to proxy the message to the host app, and send the request to the /proxy/api/messages endpoint of our helloworld-proxy app, curl -L -X POST \"http://$PROXY_HOST:$PROXY_NODEPORT/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' {\"id\":\"6e5ef78f-9e09-42f8-8294-347ece7cd07f\",\"sender\":\"remko\",\"message\":\"Hello remko (proxy)\",\"host\":\"helloworld:8080\"}","title":"Create helloworld Proxy"},{"location":"networkpolicy/#apply-network-policy-allow-no-traffic","text":"Define the Network Policy file to deny all traffic, echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: helloworld-deny-all spec: podSelector: {} policyTypes: - Ingress - Egress' > helloworld-policy-denyall.yaml Create the Network Policy, kubectl create -f helloworld-policy-denyall.yaml -n $MY_NS networkpolicy.networking.k8s.io/helloworld-deny-all created Test both the helloworld and the helloworld-proxy apps, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' curl: (7) Failed to connect to 52.118.3.110 port 30663: Operation timed out curl -L -X POST \"http://$PROXY_HOST:$PROXY_PORT/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: (7) Failed to connect to 169.61.252.4 port 80: Operation timed out It takes quite a long time before connections time out. All traffic is denied, despite that we have a LoadBalancer service added to each deployment, kubectl get svc -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 172.21.145.243 169.61.252.3 8080:30663/TCP 30h helloworld-proxy LoadBalancer 172.21.74.243 169.61.252.4 8080:30005/TCP 141m","title":"Apply Network Policy - Allow No Traffic"},{"location":"networkpolicy/#apply-network-policy-allow-only-traffic-from-proxy","text":"Let's allow direct ingress traffic to the helloworld app on port 8080 , but not allow traffic to the helloworld-proxy app. Define the Network Policy file, echo 'apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: name: helloworld-allow spec: selector: app == 'helloworld' types: - Ingress ingress: - action: Allow protocol: TCP source: selector: role == 'helloworld-proxy' destination: ports: - 8080' > helloworld-calico-allow.yaml Create the Network Policy, calicoctl create -f helloworld-calico-allow.yaml Successfully created 1 'GlobalNetworkPolicy' resource(s) Review the existing NetworkPolices in the project namespace, kubectl get networkpolicies -n $MY_NS NAME POD-SELECTOR AGE helloworld-deny-all <none> 3h10m Review the existing GlobalNetworkPolicies, kubectl get globalnetworkpolicies NAME AGE default.allow-all-outbound 33h default.allow-all-private-default 33h default.allow-bigfix-port 33h default.allow-icmp 33h default.allow-node-port-dnat 33h default.allow-sys-mgmt 33h default.allow-vrrp 33h default.helloworld-allow 14ms Test the helloworld and the `helloworld-proxy' apps again, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' {\"id\":\"19b0a0c0-ef05-4c91-8dd1-330e40d252ef\",\"sender\":\"remko\",\"message\":\"Hello remko (direct)\",\"host\":null} curl -L -X POST \"http://$PROXY_HOST:$PROXY_PORT/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: (7) Failed to connect to 169.61.252.4 port 80: Operation timed out","title":"Apply Network Policy - Allow Only Traffic From Proxy"},{"location":"networkpolicy/#cleanup","text":"kubectl delete globalnetworkpolicy default.helloworld-allow kubectl delete networkpolicy helloworld-deny-all","title":"Cleanup"},{"location":"nodeport/","text":"NodePort \u00b6 Pre-requisites \u00b6 Finish the Services and ClusterIP labs. NodePort \u00b6 To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. Apps inside the cluster can access a pod by using the in-cluster IP of the service or by sending a request to the name of the service. When you use the name of the service, kube-proxy looks up the name in the cluster DNS server and routes the request to the in-cluster IP address of the service. To allow external traffic into a kubernetes cluster, you need a NodePort ServiceType. If you set the type field of Service to NodePort , Kubernetes allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. A gateway router typically sits in front of the cluster and forwards packets to the node. The load balancer on the node routes requests to the pods. If a client wants to connect to our service on port 80 we can\u2019t just send packets directly to that port on the nodes\u2019 interfaces. The network that netfilter is set up to forward packets for is not easily routable from the gateway to the nodes. Therefor, kubernetes creates a bridge between these networks with something called a NodePort. A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the public IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when kubernetes creates a NodePort service, kube-proxy allocates a port in the range 30000\u201332767 and opens this port on the eth0 interface of every node (thus the name NodePort ). Connections to this port are forwarded to the service\u2019s cluster IP. Patch the existing Service for helloworld to type: NodePort using the kubectl patch command, kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"NodePort\"}}' -n $MY_NS service/helloworld patched Or apply the specification defined in the file helloworld-service-nodeport.yaml to set type: NodePort , apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld type: NodePort Apply the changes to the configuration from file, kubectl apply -f helloworld-service-nodeport.yaml -n $MY_NS Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply service/helloworld configured Describe the Service, kubectl describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: <none> Selector: app=helloworld Type: NodePort IP: 172.21.49.18 Port: <unset> 8080/TCP TargetPort: http-server/TCP NodePort: <unset> 30785/TCP Endpoints: 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> Kubernetes added a NodePort with port value 30785 in this example. You can now connect to the service via the public IP address of either worker node in the cluster and traffic gets forwarded to the service, which uses service discovery using the selector to deliver the request to the pod. With this piece in place we now have a complete pipeline for load balancing external client requests to all the nodes in the cluster. Retrieve the Public IP address of the worker nodes, ibmcloud ks workers --cluster $KS_CLUSTER_NAME OK ID Public IP Private IP Flavor State Status Zone Version kube-bvlntf2d0fe4l9hnres0-remkohdevik-default-00000145 150.238.221.67 10.38.200.125 u3c.2x4.encrypted normal Ready dal10 1.18.13_1536 Or to get the first worker node's public IP address and the NodePort of the service: PUBLIC_IP=$(ibmcloud ks workers --cluster $KS_CLUSTER_NAME --json | jq '.[0]' | jq -r '.publicIP') echo $PUBLIC_IP PORT=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PORT Test the deployment, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: text/plain' -d '{ \"sender\": \"world1\" }' {\"id\":\"f979a034-bada-41cb-8c67-fb5fd36d0db3\",\"sender\":\"remko\",\"message\":\"Hello world1 (direct)\",\"host\":null} The client connects to your application via a public IP address of a worker node and the NodePort. Each node proxies the port, kube-proxy receives the request, and forwards it to the service at the cluster IP. At this point the request matches the netfilter or iptables rules and gets redirected to the server pod. However, a LoadBalancer service is the standard way to expose a service. Go to LoadBalancer to learn more about the ServiceType LoadBalancer.","title":"NodePort"},{"location":"nodeport/#nodeport","text":"","title":"NodePort"},{"location":"nodeport/#pre-requisites","text":"Finish the Services and ClusterIP labs.","title":"Pre-requisites"},{"location":"nodeport/#nodeport_1","text":"To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. Apps inside the cluster can access a pod by using the in-cluster IP of the service or by sending a request to the name of the service. When you use the name of the service, kube-proxy looks up the name in the cluster DNS server and routes the request to the in-cluster IP address of the service. To allow external traffic into a kubernetes cluster, you need a NodePort ServiceType. If you set the type field of Service to NodePort , Kubernetes allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. A gateway router typically sits in front of the cluster and forwards packets to the node. The load balancer on the node routes requests to the pods. If a client wants to connect to our service on port 80 we can\u2019t just send packets directly to that port on the nodes\u2019 interfaces. The network that netfilter is set up to forward packets for is not easily routable from the gateway to the nodes. Therefor, kubernetes creates a bridge between these networks with something called a NodePort. A service of type NodePort is a ClusterIP service with an additional capability: it is reachable at the public IP address of the node as well as at the assigned cluster IP on the services network. The way this is accomplished is pretty straightforward: when kubernetes creates a NodePort service, kube-proxy allocates a port in the range 30000\u201332767 and opens this port on the eth0 interface of every node (thus the name NodePort ). Connections to this port are forwarded to the service\u2019s cluster IP. Patch the existing Service for helloworld to type: NodePort using the kubectl patch command, kubectl patch svc helloworld -p '{\"spec\": {\"type\": \"NodePort\"}}' -n $MY_NS service/helloworld patched Or apply the specification defined in the file helloworld-service-nodeport.yaml to set type: NodePort , apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld type: NodePort Apply the changes to the configuration from file, kubectl apply -f helloworld-service-nodeport.yaml -n $MY_NS Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply service/helloworld configured Describe the Service, kubectl describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: <none> Selector: app=helloworld Type: NodePort IP: 172.21.49.18 Port: <unset> 8080/TCP TargetPort: http-server/TCP NodePort: <unset> 30785/TCP Endpoints: 172.30.20.145:8080,172.30.20.146:8080,172.30.20.147:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> Kubernetes added a NodePort with port value 30785 in this example. You can now connect to the service via the public IP address of either worker node in the cluster and traffic gets forwarded to the service, which uses service discovery using the selector to deliver the request to the pod. With this piece in place we now have a complete pipeline for load balancing external client requests to all the nodes in the cluster. Retrieve the Public IP address of the worker nodes, ibmcloud ks workers --cluster $KS_CLUSTER_NAME OK ID Public IP Private IP Flavor State Status Zone Version kube-bvlntf2d0fe4l9hnres0-remkohdevik-default-00000145 150.238.221.67 10.38.200.125 u3c.2x4.encrypted normal Ready dal10 1.18.13_1536 Or to get the first worker node's public IP address and the NodePort of the service: PUBLIC_IP=$(ibmcloud ks workers --cluster $KS_CLUSTER_NAME --json | jq '.[0]' | jq -r '.publicIP') echo $PUBLIC_IP PORT=$(kubectl get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PORT Test the deployment, curl -L -X POST \"http://$PUBLIC_IP:$PORT/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: text/plain' -d '{ \"sender\": \"world1\" }' {\"id\":\"f979a034-bada-41cb-8c67-fb5fd36d0db3\",\"sender\":\"remko\",\"message\":\"Hello world1 (direct)\",\"host\":null} The client connects to your application via a public IP address of a worker node and the NodePort. Each node proxies the port, kube-proxy receives the request, and forwards it to the service at the cluster IP. At this point the request matches the netfilter or iptables rules and gets redirected to the server pod. However, a LoadBalancer service is the standard way to expose a service. Go to LoadBalancer to learn more about the ServiceType LoadBalancer.","title":"NodePort"},{"location":"services/","text":"Services \u00b6 Create a Service \u00b6 If you did not already clone the helloworld repository to your client, do so now. git clone https://github.com/remkohdev/helloworld.git ls -al Deploy the helloworld application, cd helloworld MY_NS=my-apps kubectl create namespace $MY_NS kubectl create -f helloworld-deployment.yaml -n $MY_NS deployment.apps/helloworld created The helloworld application was deployed, that is a Kubernetes Deployment object was created. kubectl get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-6c76f57b9d-fsbfh 1/1 Running 0 10s pod/helloworld-6c76f57b9d-gwk5j 1/1 Running 0 10s pod/helloworld-6c76f57b9d-jfbrz 1/1 Running 0 10s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3/3 3 3 10s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-6c76f57b9d 3 3 3 10s The deployment created also a ReplicaSet with 3 replicas of the pods. Because we did not create a Service for the helloworld containers running in pods, they cannot yet be easily accessed. When a Pod is deployed to a worker node, it is assigned a private IP address in the 172.30.0.0/16 range. Worker nodes and pods can securely communicate on the private network by using private IP addresses. However, Kubernetes creates and destroys Pods dynamically, which means that the location of the Pods changes dynamically. When a Pod is destroyed or a worker node needs to be re-created, a new private IP address is assigned. With a Service object, you can use built-in Kubernetes service discovery to expose Pods. A Service defines a set of Pods and a policy to access those Pods. Kubernetes assigns a single DNS name for a set of Pods and can load balance across Pods. When you create a Service, a set of pods and EndPoints are created to manage access to the pods. The Endpoints object in Kubernetes is the list of IP and port addresses and are created automatically when a Service is created and configured with the pods matching the selector of the Service. A Service can be configured without a selector, in that case Kubernetes does not create an associated Endpoints object. Let's look at the declaration for the helloworld-service.yaml , cat helloworld-service.yaml apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld The spec defines a few important attributes: labels , selector and port . The set of Pods that a Service targets, is determined by the selector and labels. When a Service has no selector, the corresponding Endpoints object is not created automatically. This can be useful in cases where you want to define an Endpoint manually, for instance in the case of an external database instance. The Service maps the incoming port to a targetPort . By default the targetPort is set to the same value as the incoming port field. A port definition in Pods can have a name, and you can reference these names in the targetPort attribute of a Service instead of the port number. In the Service example of helloworld , the helloworld-deployment.yaml file should have the corresponding port defined that the Service references by name, cat helloworld-deployment.yaml ports: - name: http-server containerPort: 8080 This even works for pods available via the same network protocol on different port numbers. Kubernetes supports multiple port definitions on a Service object. The default protocol for Services is TCP, see other supported protocols . ServiceTypes \u00b6 Before you create a Service for the helloworld application, let's first understand what types of services exist. Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default type is ClusterIP . To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. ServiceType values and their behaviors are: ClusterIP : Exposes the Service on a cluster-internal IP. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting : . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record. You can also use Ingress in place of Service to expose HTTP/HTTPS Services. Ingress however is technically not a ServiceType, but it acts as the entry point for your cluster and lets you consolidate routing rules into a single resource. Next, go to ClusterIP to learn more about ServiceType ClusterIP.","title":"Services"},{"location":"services/#services","text":"","title":"Services"},{"location":"services/#create-a-service","text":"If you did not already clone the helloworld repository to your client, do so now. git clone https://github.com/remkohdev/helloworld.git ls -al Deploy the helloworld application, cd helloworld MY_NS=my-apps kubectl create namespace $MY_NS kubectl create -f helloworld-deployment.yaml -n $MY_NS deployment.apps/helloworld created The helloworld application was deployed, that is a Kubernetes Deployment object was created. kubectl get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-6c76f57b9d-fsbfh 1/1 Running 0 10s pod/helloworld-6c76f57b9d-gwk5j 1/1 Running 0 10s pod/helloworld-6c76f57b9d-jfbrz 1/1 Running 0 10s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3/3 3 3 10s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-6c76f57b9d 3 3 3 10s The deployment created also a ReplicaSet with 3 replicas of the pods. Because we did not create a Service for the helloworld containers running in pods, they cannot yet be easily accessed. When a Pod is deployed to a worker node, it is assigned a private IP address in the 172.30.0.0/16 range. Worker nodes and pods can securely communicate on the private network by using private IP addresses. However, Kubernetes creates and destroys Pods dynamically, which means that the location of the Pods changes dynamically. When a Pod is destroyed or a worker node needs to be re-created, a new private IP address is assigned. With a Service object, you can use built-in Kubernetes service discovery to expose Pods. A Service defines a set of Pods and a policy to access those Pods. Kubernetes assigns a single DNS name for a set of Pods and can load balance across Pods. When you create a Service, a set of pods and EndPoints are created to manage access to the pods. The Endpoints object in Kubernetes is the list of IP and port addresses and are created automatically when a Service is created and configured with the pods matching the selector of the Service. A Service can be configured without a selector, in that case Kubernetes does not create an associated Endpoints object. Let's look at the declaration for the helloworld-service.yaml , cat helloworld-service.yaml apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld The spec defines a few important attributes: labels , selector and port . The set of Pods that a Service targets, is determined by the selector and labels. When a Service has no selector, the corresponding Endpoints object is not created automatically. This can be useful in cases where you want to define an Endpoint manually, for instance in the case of an external database instance. The Service maps the incoming port to a targetPort . By default the targetPort is set to the same value as the incoming port field. A port definition in Pods can have a name, and you can reference these names in the targetPort attribute of a Service instead of the port number. In the Service example of helloworld , the helloworld-deployment.yaml file should have the corresponding port defined that the Service references by name, cat helloworld-deployment.yaml ports: - name: http-server containerPort: 8080 This even works for pods available via the same network protocol on different port numbers. Kubernetes supports multiple port definitions on a Service object. The default protocol for Services is TCP, see other supported protocols .","title":"Create a Service"},{"location":"services/#servicetypes","text":"Before you create a Service for the helloworld application, let's first understand what types of services exist. Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default type is ClusterIP . To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. ServiceType values and their behaviors are: ClusterIP : Exposes the Service on a cluster-internal IP. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting : . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record. You can also use Ingress in place of Service to expose HTTP/HTTPS Services. Ingress however is technically not a ServiceType, but it acts as the entry point for your cluster and lets you consolidate routing rules into a single resource. Next, go to ClusterIP to learn more about ServiceType ClusterIP.","title":"ServiceTypes"},{"location":"setup/","text":"Setup \u00b6 Prerequirements \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Kubernetes cluster v1.18: for labs in Services , ClusterIP , and NodePort , you need a Kubernetes cluster with at least 1 worker node. for labs in LoadBalancer and Ingress you need a Kubernetes cluster with at least 2 worker nodes. for labs in Network Policy and Calico , you need a Kubernetes cluster with at least 2 worker nodes. for labs in VPC Gen2 , you need a Kubernetes cluster with at least 1 worker node. Setup Kubernetes Cluster \u00b6 To create a free IBM Cloud Kubernetes Service (IKS) with 1 worker node, you can upgrade the free IBM Cloud account to a free Pay-As-You-Go account. To upgrade, go here . To create a free IBM Cloud Kubernetes Service (IKS) cluster with 1 worker node, To grant permissions to an existing Kubernetes cluster, as part of an IBM managed workshop, you can access an existing cluster using your IBM Cloud account and IBM Id. To grant access permissions to a cluster, go here . To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here . Setup Client Terminal \u00b6 I recommended using the browser based client terminal that is party of the Labs environment at CognitiveLabs. to access a client terminal at CognitiveLabs.ai, go here . Alternatively, you can also use the IBM Cloud shell with tools pre-installed to run the labs at https://shell.cloud.ibm.com. The client terminal needs to have Docker , kubectl , for OpenShift you need the oc CLI, the IBM Cloud CLI and the IBM Cloud Kubernetes Service plug-in . Install Calicoctl \u00b6 Install calicoctl, $ curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl $ chmod +x calicoctl $ echo 'export PATH=$HOME:$PATH' > .bash_profile $ source .bash_profile $ ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME --admin --network With Docker container, docker pull calico/ctl:v3.17.1 As kubectl plugin, curl -o kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x kubectl-calico kubectl calico -h Login to IBM Cloud \u00b6 Log in to your cluster, e.g. if created in the us-south region, IBMID=<your IBMID email> ibmcloud login -u $IBMID If you are using federated SSO login, use the -sso flag instead. Select the account in which the cluster was created. Optionally, list all clusters and set the KS_CLUSTER_NAME environment variable to the correct cluster name, ibmcloud ks clusters KS_CLUSTER_NAME=<your cluster name> MY_NS=my-apps Download the cluster configuration to the client, ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME kubectl config current-context The config should be set to a clustername/clusterid pair, HelloWorld App \u00b6 From the Cloud shell, clone the guestbook application, $ git clone https://github.com/remkohdev/helloworld.git $ ls -al","title":"Setup"},{"location":"setup/#setup","text":"","title":"Setup"},{"location":"setup/#prerequirements","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Kubernetes cluster v1.18: for labs in Services , ClusterIP , and NodePort , you need a Kubernetes cluster with at least 1 worker node. for labs in LoadBalancer and Ingress you need a Kubernetes cluster with at least 2 worker nodes. for labs in Network Policy and Calico , you need a Kubernetes cluster with at least 2 worker nodes. for labs in VPC Gen2 , you need a Kubernetes cluster with at least 1 worker node.","title":"Prerequirements"},{"location":"setup/#setup-kubernetes-cluster","text":"To create a free IBM Cloud Kubernetes Service (IKS) with 1 worker node, you can upgrade the free IBM Cloud account to a free Pay-As-You-Go account. To upgrade, go here . To create a free IBM Cloud Kubernetes Service (IKS) cluster with 1 worker node, To grant permissions to an existing Kubernetes cluster, as part of an IBM managed workshop, you can access an existing cluster using your IBM Cloud account and IBM Id. To grant access permissions to a cluster, go here . To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here .","title":"Setup Kubernetes Cluster"},{"location":"setup/#setup-client-terminal","text":"I recommended using the browser based client terminal that is party of the Labs environment at CognitiveLabs. to access a client terminal at CognitiveLabs.ai, go here . Alternatively, you can also use the IBM Cloud shell with tools pre-installed to run the labs at https://shell.cloud.ibm.com. The client terminal needs to have Docker , kubectl , for OpenShift you need the oc CLI, the IBM Cloud CLI and the IBM Cloud Kubernetes Service plug-in .","title":"Setup Client Terminal"},{"location":"setup/#install-calicoctl","text":"Install calicoctl, $ curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl $ chmod +x calicoctl $ echo 'export PATH=$HOME:$PATH' > .bash_profile $ source .bash_profile $ ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME --admin --network With Docker container, docker pull calico/ctl:v3.17.1 As kubectl plugin, curl -o kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x kubectl-calico kubectl calico -h","title":"Install Calicoctl"},{"location":"setup/#login-to-ibm-cloud","text":"Log in to your cluster, e.g. if created in the us-south region, IBMID=<your IBMID email> ibmcloud login -u $IBMID If you are using federated SSO login, use the -sso flag instead. Select the account in which the cluster was created. Optionally, list all clusters and set the KS_CLUSTER_NAME environment variable to the correct cluster name, ibmcloud ks clusters KS_CLUSTER_NAME=<your cluster name> MY_NS=my-apps Download the cluster configuration to the client, ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME kubectl config current-context The config should be set to a clustername/clusterid pair,","title":"Login to IBM Cloud"},{"location":"setup/#helloworld-app","text":"From the Cloud shell, clone the guestbook application, $ git clone https://github.com/remkohdev/helloworld.git $ ls -al","title":"HelloWorld App"},{"location":"vpcgen2/","text":"Airgap and Secure a Kubernetes Cluster with VPC Gen2 \u00b6 This tutorial explains how to air-gap and secure an OpenShift or Kubernetes cluster and physically isolate our cluster network. Measured by strict industry specific controls for airgapping a network, like the Financial Services industry, this tutorial does not airgap the cluster. To airgap a cluster by more strict standards additional measures are needed. For more information about air-gap , go here . Prerequirements \u00b6 Free IBM Cloud account, for new account registration go here , Upgrade to free Pay-As-You-Go account, for upgrade go here , Client terminal at CognitiveClass.ai, for setup go here . Airgap \u00b6 Using an air gapped cluster is one of the foundational best practices for creating secure container deployments. You can air gap your cluster by creating a Virtual Private Cloud (VPC) and add rules to a Security Group of an Application Load Balancer (ALB) to allow certain inbound traffic where needed or add a gateway like API Connect to the VPC and expose the gateway to manage traffic, for instance with rate limit and API key access control. With Red Hat OpenShift Kubernetes (ROKS) or IBM Cloud Kubernetes Service (IKS) on VPC Generation 2 Computeon IBM Cloud, you can create an OpenShift or Kubernetes cluster on a Virtual Private Cloud (VPC) infrastructure in a matter of minutes. OpenShift is the more enterprise-ready secure Container Orchestration (CO) platform, but in this tutorial I use a plain managed Kubernetes service because for most beginners this will be more accessible and affordable. If you want however, you can choose to use OpenShift instead. This tutorial uses a free IBM Cloud account, free IKS service on IBM Cloud with a free Pay-As-You-Go account. This tutorial is based in parts on the official documentation for creating a cluster in your Virtual Private Cloud (VPC) on generation 2 compute. Steps: Setup, (Skip) Using UI to Create a Cluster with VPC Generation 2 Compute Using CLI to Create a Cluster with VPC Generation 2 Compute Create a Kubernetes Cluster Deploy the Guestbook Application, Update the Security Group Understanding the Load Balancer Ingress Application Load Balancer (ALB) Cleanup Setup \u00b6 Add the VPC infrastructure service plug-in to the IBM Cloud CLI in the client terminal, ibmcloud plugin install infrastructure-service Set the following environment variables to be used in the remainder of the tutorial. Set the USERNAME to a unique value shorter than 10 characters and set the IBMID to the email you used to create the IBM Cloud account. The other variables don't need to be changed but you can choose to rename them if you prefer. USERNAME=<bnewell> IBMID=<b.newell2@remkoh.dev> MY_REGION=us-south MY_ZONE=$MY_REGION-1 MY_VPC_NAME=$USERNAME-vpcgen2-vpc1 MY_VPC_SUBNET_NAME=$USERNAME-vpcsubnet1 MY_PUBLIC_GATEWAY=$USERNAME-public-gateway1 MY_CLUSTER_NAME=$USERNAME-iks118-vpc-cluster1 KS_VERSION=1.18 MY_NAMESPACE=my-guestbook Log in to the IBM Cloud account, if you use a Single Sign-On (SSO) provider, use the --sso flag instead of the username flag. ibmcloud login -u $IBMID [--sso] Target the region where you want to create your VPC environment. The resource group flag is optional, if you happen to know it, you can target it explicitly, but this tutorial does not use it. ibmcloud target -r $MY_REGION [-g <resource_group>] The VPC must be set up in the same multi-zone metro location where you want to create your cluster. Target generation 2 infrastructure. ibmcloud is target --gen 2 Using UI to Create a Cluster with VPC Generation 2 Compute \u00b6 This section uses the browser to create the cluster and VPC. I always prefer the command line or CLI instead because this forces me to write the commands that allow me to automate the whole process and to think of everything as code. If you want to use the CLI, skip to the next section. Using the UI, create an IBM Cloud Kubernetes Service (IKS), for infrastructure provider choose Generation 2 Compute instead of Classic. For a comparison of infrastructure providers, read Supported Infrastructure Providers, or read an Overview of VPC Networking in IBM Cloud Kubernetes Service (IKS). (dd. December 2020) This will setup the following infrastructure for you: Virtual Private Cloud, watch this good primer on VPC by Ryan Sumner, Subnet, Security Group, an Access Control List (ACL), Public Gateway with a Floating IP for Public Access, an encrypted VPN Gateway or a Direct Link Private Circuits for Private Access, an elastic Load Balancer for VPC, using an Application Load Balancer (ALB), which includes Sysdig monitoring, Hihg Availability (HA) with a Domain Name Server (DNS), Multi-Zone Region (MZR) support, L4 network layer and L7 application layer support, both public and private load balancing. Using CLI to Create a Cluster with VPC Generation 2 Compute \u00b6 This section uses the IBM Cloud CLI to create a cluster in your VPC on generation 2 compute. Create a VPC and a subnet, ibmcloud is vpc-create $MY_VPC_NAME ibmcloud is vpcs ID Name Status Classic access Default network ACL Default security group Resource group r006\u20133883b334\u20131f4e-4d6a-b3b7\u2013343a029d81bc remkohdev-vpcgen2-vpc1 available false prowling-prevail-universe-equivocal hatracks-fraying-unloader-prevail default MY_VPC_ID=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .id ') echo $MY_VPC_ID ibmcloud is subnet-create $MY_VPC_SUBNET_NAME $MY_VPC_ID --zone $MY_ZONE --ipv4-address-count 256 MY_VPC_SUBNET_ID=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .id ') echo $MY_VPC_SUBNET_ID Inspect the VPC's default security group, MY_DEFAULT_SG_NAME=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .default_security_group.name ') echo $MY_DEFAULT_SG_NAME MY_DEFAULT_SG_ID=$(ibmcloud is security-groups --output json | jq -r '.[] | select( .name=='\\\"$MY_DEFAULT_SG_NAME\\\"') | .id ') echo $MY_DEFAULT_SG_ID ibmcloud is security-group $MY_DEFAULT_SG_ID --output json { \"created_at\": \"2020-12-18T22:54:04.000Z\", \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::security-group:r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\", \"network_interfaces\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_2ba757dd-0d40-48c6-b72e-6fb2ea12584a/network_interfaces/0717-57f73998-6af6-43e1-981f-698f021f4842\", \"id\": \"0717-57f73998-6af6-43e1-981f-698f021f4842\", \"name\": \"unpleased-urethane-recycled-subduing\", \"primary_ipv4_address\": \"10.240.0.4\", \"resource_type\": \"network_interface\" }, { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_3affef2d-c4b7-49e0-a785-c65ad2f64a35/network_interfaces/0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"id\": \"0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"name\": \"sweep-timpani-unable-stricken\", \"primary_ipv4_address\": \"10.240.0.5\", \"resource_type\": \"network_interface\" } ], \"resource_group\": { \"href\": \"https://resource-controller.cloud.ibm.com/v2/resource_groups/fdd290732f7d47909181a189494e2990\", \"id\": \"fdd290732f7d47909181a189494e2990\", \"name\": \"default\" }, \"rules\": [ { \"direction\": \"outbound\", \"id\": \"r006-c0b88082-748c-47a8-bc75-c9aabb3da4e3\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-4693fcb0-2998-4d73-b62f-9dd80fe1ac56\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\" } }, { \"direction\": \"inbound\", \"id\": \"r006-b0fb17d4-b858-4a1f-bb8c-24231f527155\", \"ip_version\": \"ipv4\", \"port_max\": 22, \"port_min\": 22, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-87a1f11b-6d30-4d63-84df-1bda1c5e25f2\", \"ip_version\": \"ipv4\", \"protocol\": \"icmp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" }, \"type\": 8 }, { \"direction\": \"inbound\", \"id\": \"r006-ade5f825-49e0-47bf-b6cf-b020b5fa23b8\", \"ip_version\": \"ipv4\", \"port_max\": 80, \"port_min\": 80, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } } ], \"targets\": null, \"vpc\": { \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::vpc:r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/vpcs/r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"id\": \"r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"name\": \"remkohdev-vpcgen2-vpc1\" } } Create and attach a public gateway, this should create a floating IP for your public gateway attached to your subnet, ibmcloud is public-gateway-create $MY_PUBLIC_GATEWAY $MY_VPC_ID $MY_ZONE MY_PUBLIC_GATEWAY_ID=$(ibmcloud is public-gateways --output json | jq -r '.[] | select( .name=='\\\"$MY_PUBLIC_GATEWAY\\\"') | .id') echo $MY_PUBLIC_GATEWAY_ID MY_PUBLIC_GATEWAY_ID2=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .public_gateway.id ') echo $MY_PUBLIC_GATEWAY_ID2 ibmcloud is subnet-update $MY_VPC_SUBNET_ID --public-gateway-id $MY_PUBLIC_GATEWAY_ID MY_FLOATING_IP=$(ibmcloud is subnet-public-gateway $MY_VPC_SUBNET_ID --output json | jq -r '.floating_ip.address') echo $MY_FLOATING_IP Go to your subnets for VPC, click the linked subnet name you just created to view the new subnet details, it should include a floating IP, Create a Kubernetes Cluster \u00b6 Create a cluster in your VPC in the same zone as the subnet. By default, your cluster is created with a public and a private service endpoint. You can use the public service endpoint to access the Kubernetes master, ibmcloud ks zone ls --provider vpc-gen2 ibmcloud ks versions ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID Creating cluster... OK Cluster created with ID bvglln7d0e5j0u9lfa80 Review your IBM Cloud account resources, Click the linked cluster name of the cluster you just created. If you do not see the cluster listed yet, wait and refresh the page. Check the status of the new cluster, ibmcloud ks clusters MY_CLUSTER_ID=$(ibmcloud ks clusters --output json --provider vpc-gen2 | jq -r '.[] | select( .name=='\\\"$MY_CLUSTER_NAME\\\"') | .id ') Deploy the Guestbook Application \u00b6 Connect to your cluster to set the current-context, ibmcloud ks cluster config --cluster $MY_CLUSTER_ID kubectl config current-context Deploy the guestbook application, kubectl create namespace $MY_NAMESPACE kubectl create deployment guestbook --image=ibmcom/guestbook:v1 -n $MY_NAMESPACE kubectl expose deployment guestbook --type=\"LoadBalancer\" --port=3000 --target-port=3000 -n $MY_NAMESPACE kubectl get svc -n $MY_NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.48.26 7a6a66a7-us-south.lb.appdomain.cloud 3000:32308/TCP 31m SVC_EXTERNAL_IP=$(kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[] | .status.loadBalancer.ingress[0].hostname ') echo $SVC_EXTERNAL_IP SVC_NODEPORT=$(kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .nodePort') echo $SVC_NODEPORT SVC_PORT=$(kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .port') echo $SVC_PORT Try to send a request to the guestbook application, curl http://$SVC_EXTERNAL_IP:$SVC_PORT curl: (52) Empty reply from server Update the Security Group \u00b6 To allow any traffic to applications that are deployed on your cluster's worker nodes, you have to modify the VPC's default security group by ID. Update security group and add an inbound rule for NodePort of the service you created when exposing the guestbook deployment, ibmcloud is security-group-rule-add $MY_DEFAULT_SG_ID inbound tcp --port-min $SVC_NODEPORT --port-max $SVC_NODEPORT Creating rule for security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 Direction inbound IP version ipv4 Protocol tcp Min destination port 32308 Max destination port 32308 Remote 0.0.0.0/0 ibmcloud is security-group-rules $MY_DEFAULT_SG_ID Listing rules of security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID Direction IP version Protocol Remote r006-6c281868-cfbd-46d1-b714-81e085ae2b85 outbound ipv4 all 0.0.0.0/0 r006-b07bbf77-84a0-4e35-90b7-5422f035cf1e inbound ipv4 all compacted-imprison-clinic-support r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 inbound ipv4 tcp Ports:Min=32308,Max=32308 0.0.0.0/0 Or add a security group rule to allow inbound TCP traffic on all Kubernetes ports in the range of 30000\u201332767. Try again to reach the guestbook application, curl http://$SVC_EXTERNAL_IP:$SVC_PORT You should now be able to see the HTML response object from the Guestbook application. Open the guestbook URL in a browser to review the web page. Understanding the Load Balancer \u00b6 Right now, the most important thing is that our tutorial works. Oof. You secured your Kubernetes cluster with a Virtual Private Cloud (VPC) and essentially so-called air-gapped the cluster, blocking direct access to your cluster. To better understand what exactly happened in the end, especially to see why we are using the port of the Service object instead of the NodePort which is the port we allowed inbound traffic on, let's inspect the Service resource of the Guestbook deployment in more detail. kubectl get svc -n $MY_NAMESPACE --output json { \"apiVersion\": \"v1\", \"items\": [ { \"apiVersion\": \"v1\", \"kind\": \"Service\", \"metadata\": { \"creationTimestamp\": \"2020-12-23T03:08:27Z\", \"finalizers\": [ \"service.kubernetes.io/load-balancer-cleanup\" ], \"labels\": { \"app\": \"guestbook\" }, \"managedFields\": [ { \"apiVersion\": \"v1\", \"fieldsType\": \"FieldsV1\", \"fieldsV1\": { \"f:metadata\": { \"f:labels\": { \".\": {}, \"f:app\": {} } }, \"f:spec\": { \"f:externalTrafficPolicy\": {}, \"f:ports\": { \".\": {}, \"k:{\\\"port\\\":3000,\\\"protocol\\\":\\\"TCP\\\"}\": { \".\": {}, \"f:port\": {}, \"f:protocol\": {}, \"f:targetPort\": {} } }, \"f:selector\": { \".\": {}, \"f:app\": {} }, \"f:sessionAffinity\": {}, \"f:type\": {} } }, \"manager\": \"kubectl-expose\", \"operation\": \"Update\", \"time\": \"2020-12-23T03:08:26Z\" }, { \"apiVersion\": \"v1\", \"fieldsType\": \"FieldsV1\", \"fieldsV1\": { \"f:metadata\": { \"f:finalizers\": { \".\": {}, \"v:\\\"service.kubernetes.io/load-balancer-cleanup\\\"\": {} } }, \"f:status\": { \"f:loadBalancer\": { \"f:ingress\": {} } } }, \"manager\": \"ibm-cloud-controller-manager\", \"operation\": \"Update\", \"time\": \"2020-12-23T03:29:13Z\" } ], \"name\": \"guestbook\", \"namespace\": \"my-guestbook\", \"resourceVersion\": \"6502\", \"selfLink\": \"/api/v1/namespaces/my-guestbook/services/guestbook\", \"uid\": \"22bf711e-08e0-4827-8299-53a008b52481\" }, \"spec\": { \"clusterIP\": \"172.21.48.26\", \"externalTrafficPolicy\": \"Cluster\", \"ports\": [ { \"nodePort\": 32308, \"port\": 3000, \"protocol\": \"TCP\", \"targetPort\": 3000 } ], \"selector\": { \"app\": \"guestbook\" }, \"sessionAffinity\": \"None\", \"type\": \"LoadBalancer\" }, \"status\": { \"loadBalancer\": { \"ingress\": [ { \"hostname\": \"7a6a66a7-us-south.lb.appdomain.cloud\" } ] } } } ], \"kind\": \"List\", \"metadata\": { \"resourceVersion\": \"\", \"selfLink\": \"\" } } Note: Service resource lists a property status.loadBalancer.ingress with hostname. We used the hostname to access the Guestbook Service. spec.ports.port is the port on which the service listens for external network traffic. The externalTrafficPolicy property is set to Cluster, which always routes traffic to all pods running a service with equal distribution. we used a Service type of LoadBalancer. Ingress Application Load Balancer (ALB) \u00b6 Ingress is a Kubernetes service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps by using a unique public or private route. The Ingress application load balancer (ALB) is a layer 7 (L7) load balancer, which implements the NGINX Ingress controller. A layer 4 (L4) LoadBalancer service exposes the ALB so that the ALB can receive external requests that come into your cluster. An Ingress deployment consists of three components: Ingress resources, an internal L7 Application Load Balancer (ALB), an external L4 load balancer to handle incoming requests across zones. For classic clusters, this component is the Multi-Zone Load Balancer (MZLB) that IBM Cloud Kubernetes Service creates for you. For VPC clusters, this component is the VPC load balancer created in your VPC. To expose an app by using Ingress, you must create a Kubernetes service for your app and register this service with Ingress by defining an Ingress resource. (To learn how, go to Ingress and ALB ). The following diagram shows how Ingress directs communication from the internet to an app in a VPC multizone cluster. source A VPC load balancer listens for external traffic, and based on the resolved IP address, the VPC load balancer sends the request to an available Application Load Balancer (ALB). The Application Load Balancer (ALB) listens for incoming HTTP, HTTPS, or TCP service requests, checks if routing rules exist for the application, and then forwards requests to the appropriate app pod according to the rules defined in the Ingress resource. In the IKS instance, there is a private and a public Ingress ALB installed, the private Ingress ALB is disabled. ibmcloud ks ingress alb ls -c $MY_CLUSTER_NAME OK ALB ID Enabled State Type Load Balancer Hostname Zone Build Status private-crbvhau5gd0do97g1vvo50-alb1 false disabled private - us-south-1 ingress:/ingress-auth: - public-crbvhau5gd0do97g1vvo50-alb1 true enabled public 1e7d00e2-us-south.lb.appdomain.cloud us-south-1 ingress:0.35.0_826_iks/ingress-auth: - List the VPC Load Balancers that were created, ibmcloud is load-balancers Listing load balancers for generation 2 compute in all resource groups and region us-south under account Remko de Knikker as user b.newell2@remkoh.dev... ID Name Family Subnets Is public Provision status Operating status Resource group r006-1e7d00e2-cc50-4f5b-a12f-b5bcfa95c439 kube-bvhau5gd0do97g1vvo50-c974a95bc72740fba2839a191ba23e17 Application remkohdev-vpcsubnet1 true active online Default r006-7a6a66a7-9871-473e-b105-20640caa0f77 kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481 Application remkohdev-vpcsubnet1 true active online Default The Family property is listed as value Application (Application Load Balancer (ALB)) and the Name is listed as value dynamic . This means that IBM Cloud Application Load Balancer for VPC integrates with instance groups, which can auto scale your back-end members. Pool members are dynamically added and deleted based on your usage and requirements. Round-robin is the default load-balancing method, but you can also use weighted round-robin or least connections. Inspect details for each VPC ALB and inspect Listeners, Pools and Pool Members, MY_LOAD_BALANCER_ID=r006-7a6a66a7-9871-473e-b105-20640caa0f77 ibmcloud is load-balancer $MY_LOAD_BALANCER_ID --output json { \"created_at\": \"2020-12-23T03:29:11.831Z\", \"crn\": \"crn:v1:bluemix:public:is:us-south:a/31296e3a285f42fdadd51ce14beba65e::load-balancer:r006-7a6a66a7-9871-473e-b105-20640caa0f77\", \"hostname\": \"7a6a66a7-us-south.lb.appdomain.cloud\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77\", \"id\": \"r006-7a6a66a7-9871-473e-b105-20640caa0f77\", \"is_public\": true, \"listeners\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\", \"id\": \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" } ], \"logging\": { \"datapath\": { \"active\": false } }, \"name\": \"kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481\", \"operating_status\": \"online\", \"pools\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"id\": \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"name\": \"tcp-3000-32308\" } ], \"private_ips\": [ { \"address\": \"10.240.0.7\" }, { \"address\": \"10.240.0.8\" } ], \"profile\": { \"family\": \"Application\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancer/profiles/dynamic\", \"name\": \"dynamic\" }, \"provisioning_status\": \"active\", \"public_ips\": [ { \"address\": \"52.116.134.164\" }, { \"address\": \"52.116.142.235\" } ], \"resource_group\": { \"href\": \"https://resource-controller.cloud.ibm.com/v1/resource_groups/68af6383f717459686457a6434c4d19f\", \"id\": \"68af6383f717459686457a6434c4d19f\", \"name\": \"Default\" }, \"security_groups\": null, \"subnets\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/subnets/0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\", \"id\": \"0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\", \"name\": \"remkohdev-vpcsubnet1\" } ] } MYLOAD_BALANCER_LISTENER_ID=r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c ibmcloud is load-balancer-listener $MY_LOAD_BALANCER_ID $MYLOAD_BALANCER_LISTENER_ID --output json { \"created_at\": \"2020-12-23T03:29:12.078Z\", \"default_pool\": { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"id\": \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"name\": \"tcp-3000-32308\" }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\", \"id\": \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\", \"policies\": null, \"port\": 3000, \"protocol\": \"tcp\", \"provisioning_status\": \"active\" } ibmcloud is load-balancer-pools $MY_LOAD_BALANCER_ID --output json [ { \"algorithm\": \"round_robin\", \"created_at\": \"2020-12-23T03:29:11.957Z\", \"health_monitor\": { \"delay\": 5, \"max_retries\": 2, \"port\": 32308, \"timeout\": 2, \"type\": \"tcp\" }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"id\": \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"members\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"id\": \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"port\": null, \"target\": null } ], \"name\": \"tcp-3000-32308\", \"protocol\": \"tcp\", \"provisioning_status\": \"active\", \"proxy_protocol\": \"disabled\" } ] MY_LOAD_BALANCER_POOL_ID=r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492 ibmcloud is load-balancer-pool-members $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID --output json [ { \"created_at\": \"2020-12-23T03:29:11.981Z\", \"health\": \"ok\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"id\": \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"port\": 32308, \"provisioning_status\": \"active\", \"target\": { \"address\": \"10.240.0.4\" }, \"weight\": 50 } ] MY_LOAD_BALANCER_POOL_MEMBER_ID=r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51 ibmcloud is load-balancer-pool-member $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID $MY_LOAD_BALANCER_POOL_MEMBER_ID --output json { \"created_at\": \"2020-12-23T03:29:11.981Z\", \"health\": \"ok\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"id\": \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"port\": 32308, \"provisioning_status\": \"active\", \"target\": { \"address\": \"10.240.0.4\" }, \"weight\": 50 } Note: above, I manually copy-pasted the IDs of the VPC resources into the next command, instead of using the full jq syntax each time to retrieve the values and set corresponding environment variables. You see that the second load balancer has a single front-end listener on port 3000 forwarding to a load balancer pool member on port 32308 on IP 10.240.0.4, which is the Private IP of the single worker node of my cluster. kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.240.0.4 Ready <none> 14h v1.18.13+IKS 10.240.0.4 10.240.0.4 Ubuntu 18.04.5 LTS 4.15.0-128-generic containerd://1.3.9 From the Kubernetes docs about External Load Balancer Providers you see that: \"When the Service type is set to LoadBalancer, Kubernetes provides functionality equivalent to type equals ClusterIP to pods within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the Kubernetes pods. The Kubernetes service controller automates the creation of the external load balancer, health checks (if needed), firewall rules (if needed) and retrieves the external IP allocated by the cloud provider and populates it in the service object.\" Cleanup \u00b6 Delete your Kubernetes cluster, ibmcloud ks cluster rm --cluster $MY_CLUSTER_NAME Do you want to delete the persistent storage for this cluster? If yes, the data cannot be recovered. If no, you can delete the persistent storage later in your IBM Cloud infrastructure account. [y/N]> y After you run this command, the cluster cannot be restored. Remove the cluster remkohdev-iks118-vpc-cluster1? [y/N]> y Removing cluster remkohdev-iks118-vpc-cluster1, persistent storage... OK Delete the Gateways, Load Balancers, Network Interfaces, subnet, public gateways, and finally delete the VPC, ibmcloud is vpn-gateways ibmcloud is vpn-gateway-delete $vpnid ibmcloud is load-balancers ibmcloud is load-balancer-delete $lbid ibmcloud is instance-network-interfaces $vsi ibmcloud is instances ibmcloud is instance-delete $vsi ibmcloud is subnets ibmcloud is subnet-delete $subnet ibmcloud is public-gateways ibmcloud is public-gateway-delete $gateway ibmcloud is vpcs ibmcloud is vpc-delete $vpc Conclusion \u00b6 You're awesome! Security is an important part of any software application development and \" airgapping \" your cluster by adding a VPC Generation 2 is a first step in securing your cluster, network and containers.","title":"VPC Gen2"},{"location":"vpcgen2/#airgap-and-secure-a-kubernetes-cluster-with-vpc-gen2","text":"This tutorial explains how to air-gap and secure an OpenShift or Kubernetes cluster and physically isolate our cluster network. Measured by strict industry specific controls for airgapping a network, like the Financial Services industry, this tutorial does not airgap the cluster. To airgap a cluster by more strict standards additional measures are needed. For more information about air-gap , go here .","title":"Airgap and Secure a Kubernetes Cluster with VPC\u00a0Gen2"},{"location":"vpcgen2/#prerequirements","text":"Free IBM Cloud account, for new account registration go here , Upgrade to free Pay-As-You-Go account, for upgrade go here , Client terminal at CognitiveClass.ai, for setup go here .","title":"Prerequirements"},{"location":"vpcgen2/#airgap","text":"Using an air gapped cluster is one of the foundational best practices for creating secure container deployments. You can air gap your cluster by creating a Virtual Private Cloud (VPC) and add rules to a Security Group of an Application Load Balancer (ALB) to allow certain inbound traffic where needed or add a gateway like API Connect to the VPC and expose the gateway to manage traffic, for instance with rate limit and API key access control. With Red Hat OpenShift Kubernetes (ROKS) or IBM Cloud Kubernetes Service (IKS) on VPC Generation 2 Computeon IBM Cloud, you can create an OpenShift or Kubernetes cluster on a Virtual Private Cloud (VPC) infrastructure in a matter of minutes. OpenShift is the more enterprise-ready secure Container Orchestration (CO) platform, but in this tutorial I use a plain managed Kubernetes service because for most beginners this will be more accessible and affordable. If you want however, you can choose to use OpenShift instead. This tutorial uses a free IBM Cloud account, free IKS service on IBM Cloud with a free Pay-As-You-Go account. This tutorial is based in parts on the official documentation for creating a cluster in your Virtual Private Cloud (VPC) on generation 2 compute. Steps: Setup, (Skip) Using UI to Create a Cluster with VPC Generation 2 Compute Using CLI to Create a Cluster with VPC Generation 2 Compute Create a Kubernetes Cluster Deploy the Guestbook Application, Update the Security Group Understanding the Load Balancer Ingress Application Load Balancer (ALB) Cleanup","title":"Airgap"},{"location":"vpcgen2/#setup","text":"Add the VPC infrastructure service plug-in to the IBM Cloud CLI in the client terminal, ibmcloud plugin install infrastructure-service Set the following environment variables to be used in the remainder of the tutorial. Set the USERNAME to a unique value shorter than 10 characters and set the IBMID to the email you used to create the IBM Cloud account. The other variables don't need to be changed but you can choose to rename them if you prefer. USERNAME=<bnewell> IBMID=<b.newell2@remkoh.dev> MY_REGION=us-south MY_ZONE=$MY_REGION-1 MY_VPC_NAME=$USERNAME-vpcgen2-vpc1 MY_VPC_SUBNET_NAME=$USERNAME-vpcsubnet1 MY_PUBLIC_GATEWAY=$USERNAME-public-gateway1 MY_CLUSTER_NAME=$USERNAME-iks118-vpc-cluster1 KS_VERSION=1.18 MY_NAMESPACE=my-guestbook Log in to the IBM Cloud account, if you use a Single Sign-On (SSO) provider, use the --sso flag instead of the username flag. ibmcloud login -u $IBMID [--sso] Target the region where you want to create your VPC environment. The resource group flag is optional, if you happen to know it, you can target it explicitly, but this tutorial does not use it. ibmcloud target -r $MY_REGION [-g <resource_group>] The VPC must be set up in the same multi-zone metro location where you want to create your cluster. Target generation 2 infrastructure. ibmcloud is target --gen 2","title":"Setup"},{"location":"vpcgen2/#using-ui-to-create-a-cluster-with-vpc-generation-2-compute","text":"This section uses the browser to create the cluster and VPC. I always prefer the command line or CLI instead because this forces me to write the commands that allow me to automate the whole process and to think of everything as code. If you want to use the CLI, skip to the next section. Using the UI, create an IBM Cloud Kubernetes Service (IKS), for infrastructure provider choose Generation 2 Compute instead of Classic. For a comparison of infrastructure providers, read Supported Infrastructure Providers, or read an Overview of VPC Networking in IBM Cloud Kubernetes Service (IKS). (dd. December 2020) This will setup the following infrastructure for you: Virtual Private Cloud, watch this good primer on VPC by Ryan Sumner, Subnet, Security Group, an Access Control List (ACL), Public Gateway with a Floating IP for Public Access, an encrypted VPN Gateway or a Direct Link Private Circuits for Private Access, an elastic Load Balancer for VPC, using an Application Load Balancer (ALB), which includes Sysdig monitoring, Hihg Availability (HA) with a Domain Name Server (DNS), Multi-Zone Region (MZR) support, L4 network layer and L7 application layer support, both public and private load balancing.","title":"Using UI to Create a Cluster with VPC Generation 2\u00a0Compute"},{"location":"vpcgen2/#using-cli-to-create-a-cluster-with-vpc-generation-2-compute","text":"This section uses the IBM Cloud CLI to create a cluster in your VPC on generation 2 compute. Create a VPC and a subnet, ibmcloud is vpc-create $MY_VPC_NAME ibmcloud is vpcs ID Name Status Classic access Default network ACL Default security group Resource group r006\u20133883b334\u20131f4e-4d6a-b3b7\u2013343a029d81bc remkohdev-vpcgen2-vpc1 available false prowling-prevail-universe-equivocal hatracks-fraying-unloader-prevail default MY_VPC_ID=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .id ') echo $MY_VPC_ID ibmcloud is subnet-create $MY_VPC_SUBNET_NAME $MY_VPC_ID --zone $MY_ZONE --ipv4-address-count 256 MY_VPC_SUBNET_ID=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .id ') echo $MY_VPC_SUBNET_ID Inspect the VPC's default security group, MY_DEFAULT_SG_NAME=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .default_security_group.name ') echo $MY_DEFAULT_SG_NAME MY_DEFAULT_SG_ID=$(ibmcloud is security-groups --output json | jq -r '.[] | select( .name=='\\\"$MY_DEFAULT_SG_NAME\\\"') | .id ') echo $MY_DEFAULT_SG_ID ibmcloud is security-group $MY_DEFAULT_SG_ID --output json { \"created_at\": \"2020-12-18T22:54:04.000Z\", \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::security-group:r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\", \"network_interfaces\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_2ba757dd-0d40-48c6-b72e-6fb2ea12584a/network_interfaces/0717-57f73998-6af6-43e1-981f-698f021f4842\", \"id\": \"0717-57f73998-6af6-43e1-981f-698f021f4842\", \"name\": \"unpleased-urethane-recycled-subduing\", \"primary_ipv4_address\": \"10.240.0.4\", \"resource_type\": \"network_interface\" }, { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_3affef2d-c4b7-49e0-a785-c65ad2f64a35/network_interfaces/0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"id\": \"0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"name\": \"sweep-timpani-unable-stricken\", \"primary_ipv4_address\": \"10.240.0.5\", \"resource_type\": \"network_interface\" } ], \"resource_group\": { \"href\": \"https://resource-controller.cloud.ibm.com/v2/resource_groups/fdd290732f7d47909181a189494e2990\", \"id\": \"fdd290732f7d47909181a189494e2990\", \"name\": \"default\" }, \"rules\": [ { \"direction\": \"outbound\", \"id\": \"r006-c0b88082-748c-47a8-bc75-c9aabb3da4e3\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-4693fcb0-2998-4d73-b62f-9dd80fe1ac56\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\" } }, { \"direction\": \"inbound\", \"id\": \"r006-b0fb17d4-b858-4a1f-bb8c-24231f527155\", \"ip_version\": \"ipv4\", \"port_max\": 22, \"port_min\": 22, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-87a1f11b-6d30-4d63-84df-1bda1c5e25f2\", \"ip_version\": \"ipv4\", \"protocol\": \"icmp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" }, \"type\": 8 }, { \"direction\": \"inbound\", \"id\": \"r006-ade5f825-49e0-47bf-b6cf-b020b5fa23b8\", \"ip_version\": \"ipv4\", \"port_max\": 80, \"port_min\": 80, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } } ], \"targets\": null, \"vpc\": { \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::vpc:r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/vpcs/r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"id\": \"r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"name\": \"remkohdev-vpcgen2-vpc1\" } } Create and attach a public gateway, this should create a floating IP for your public gateway attached to your subnet, ibmcloud is public-gateway-create $MY_PUBLIC_GATEWAY $MY_VPC_ID $MY_ZONE MY_PUBLIC_GATEWAY_ID=$(ibmcloud is public-gateways --output json | jq -r '.[] | select( .name=='\\\"$MY_PUBLIC_GATEWAY\\\"') | .id') echo $MY_PUBLIC_GATEWAY_ID MY_PUBLIC_GATEWAY_ID2=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .public_gateway.id ') echo $MY_PUBLIC_GATEWAY_ID2 ibmcloud is subnet-update $MY_VPC_SUBNET_ID --public-gateway-id $MY_PUBLIC_GATEWAY_ID MY_FLOATING_IP=$(ibmcloud is subnet-public-gateway $MY_VPC_SUBNET_ID --output json | jq -r '.floating_ip.address') echo $MY_FLOATING_IP Go to your subnets for VPC, click the linked subnet name you just created to view the new subnet details, it should include a floating IP,","title":"Using CLI to Create a Cluster with VPC Generation 2\u00a0Compute"},{"location":"vpcgen2/#create-a-kubernetes-cluster","text":"Create a cluster in your VPC in the same zone as the subnet. By default, your cluster is created with a public and a private service endpoint. You can use the public service endpoint to access the Kubernetes master, ibmcloud ks zone ls --provider vpc-gen2 ibmcloud ks versions ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID Creating cluster... OK Cluster created with ID bvglln7d0e5j0u9lfa80 Review your IBM Cloud account resources, Click the linked cluster name of the cluster you just created. If you do not see the cluster listed yet, wait and refresh the page. Check the status of the new cluster, ibmcloud ks clusters MY_CLUSTER_ID=$(ibmcloud ks clusters --output json --provider vpc-gen2 | jq -r '.[] | select( .name=='\\\"$MY_CLUSTER_NAME\\\"') | .id ')","title":"Create a Kubernetes Cluster"},{"location":"vpcgen2/#deploy-the-guestbook-application","text":"Connect to your cluster to set the current-context, ibmcloud ks cluster config --cluster $MY_CLUSTER_ID kubectl config current-context Deploy the guestbook application, kubectl create namespace $MY_NAMESPACE kubectl create deployment guestbook --image=ibmcom/guestbook:v1 -n $MY_NAMESPACE kubectl expose deployment guestbook --type=\"LoadBalancer\" --port=3000 --target-port=3000 -n $MY_NAMESPACE kubectl get svc -n $MY_NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.48.26 7a6a66a7-us-south.lb.appdomain.cloud 3000:32308/TCP 31m SVC_EXTERNAL_IP=$(kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[] | .status.loadBalancer.ingress[0].hostname ') echo $SVC_EXTERNAL_IP SVC_NODEPORT=$(kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .nodePort') echo $SVC_NODEPORT SVC_PORT=$(kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .port') echo $SVC_PORT Try to send a request to the guestbook application, curl http://$SVC_EXTERNAL_IP:$SVC_PORT curl: (52) Empty reply from server","title":"Deploy the Guestbook Application"},{"location":"vpcgen2/#update-the-security-group","text":"To allow any traffic to applications that are deployed on your cluster's worker nodes, you have to modify the VPC's default security group by ID. Update security group and add an inbound rule for NodePort of the service you created when exposing the guestbook deployment, ibmcloud is security-group-rule-add $MY_DEFAULT_SG_ID inbound tcp --port-min $SVC_NODEPORT --port-max $SVC_NODEPORT Creating rule for security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 Direction inbound IP version ipv4 Protocol tcp Min destination port 32308 Max destination port 32308 Remote 0.0.0.0/0 ibmcloud is security-group-rules $MY_DEFAULT_SG_ID Listing rules of security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID Direction IP version Protocol Remote r006-6c281868-cfbd-46d1-b714-81e085ae2b85 outbound ipv4 all 0.0.0.0/0 r006-b07bbf77-84a0-4e35-90b7-5422f035cf1e inbound ipv4 all compacted-imprison-clinic-support r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 inbound ipv4 tcp Ports:Min=32308,Max=32308 0.0.0.0/0 Or add a security group rule to allow inbound TCP traffic on all Kubernetes ports in the range of 30000\u201332767. Try again to reach the guestbook application, curl http://$SVC_EXTERNAL_IP:$SVC_PORT You should now be able to see the HTML response object from the Guestbook application. Open the guestbook URL in a browser to review the web page.","title":"Update the Security\u00a0Group"},{"location":"vpcgen2/#understanding-the-load-balancer","text":"Right now, the most important thing is that our tutorial works. Oof. You secured your Kubernetes cluster with a Virtual Private Cloud (VPC) and essentially so-called air-gapped the cluster, blocking direct access to your cluster. To better understand what exactly happened in the end, especially to see why we are using the port of the Service object instead of the NodePort which is the port we allowed inbound traffic on, let's inspect the Service resource of the Guestbook deployment in more detail. kubectl get svc -n $MY_NAMESPACE --output json { \"apiVersion\": \"v1\", \"items\": [ { \"apiVersion\": \"v1\", \"kind\": \"Service\", \"metadata\": { \"creationTimestamp\": \"2020-12-23T03:08:27Z\", \"finalizers\": [ \"service.kubernetes.io/load-balancer-cleanup\" ], \"labels\": { \"app\": \"guestbook\" }, \"managedFields\": [ { \"apiVersion\": \"v1\", \"fieldsType\": \"FieldsV1\", \"fieldsV1\": { \"f:metadata\": { \"f:labels\": { \".\": {}, \"f:app\": {} } }, \"f:spec\": { \"f:externalTrafficPolicy\": {}, \"f:ports\": { \".\": {}, \"k:{\\\"port\\\":3000,\\\"protocol\\\":\\\"TCP\\\"}\": { \".\": {}, \"f:port\": {}, \"f:protocol\": {}, \"f:targetPort\": {} } }, \"f:selector\": { \".\": {}, \"f:app\": {} }, \"f:sessionAffinity\": {}, \"f:type\": {} } }, \"manager\": \"kubectl-expose\", \"operation\": \"Update\", \"time\": \"2020-12-23T03:08:26Z\" }, { \"apiVersion\": \"v1\", \"fieldsType\": \"FieldsV1\", \"fieldsV1\": { \"f:metadata\": { \"f:finalizers\": { \".\": {}, \"v:\\\"service.kubernetes.io/load-balancer-cleanup\\\"\": {} } }, \"f:status\": { \"f:loadBalancer\": { \"f:ingress\": {} } } }, \"manager\": \"ibm-cloud-controller-manager\", \"operation\": \"Update\", \"time\": \"2020-12-23T03:29:13Z\" } ], \"name\": \"guestbook\", \"namespace\": \"my-guestbook\", \"resourceVersion\": \"6502\", \"selfLink\": \"/api/v1/namespaces/my-guestbook/services/guestbook\", \"uid\": \"22bf711e-08e0-4827-8299-53a008b52481\" }, \"spec\": { \"clusterIP\": \"172.21.48.26\", \"externalTrafficPolicy\": \"Cluster\", \"ports\": [ { \"nodePort\": 32308, \"port\": 3000, \"protocol\": \"TCP\", \"targetPort\": 3000 } ], \"selector\": { \"app\": \"guestbook\" }, \"sessionAffinity\": \"None\", \"type\": \"LoadBalancer\" }, \"status\": { \"loadBalancer\": { \"ingress\": [ { \"hostname\": \"7a6a66a7-us-south.lb.appdomain.cloud\" } ] } } } ], \"kind\": \"List\", \"metadata\": { \"resourceVersion\": \"\", \"selfLink\": \"\" } } Note: Service resource lists a property status.loadBalancer.ingress with hostname. We used the hostname to access the Guestbook Service. spec.ports.port is the port on which the service listens for external network traffic. The externalTrafficPolicy property is set to Cluster, which always routes traffic to all pods running a service with equal distribution. we used a Service type of LoadBalancer.","title":"Understanding the Load\u00a0Balancer"},{"location":"vpcgen2/#ingress-application-load-balancer-alb","text":"Ingress is a Kubernetes service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps by using a unique public or private route. The Ingress application load balancer (ALB) is a layer 7 (L7) load balancer, which implements the NGINX Ingress controller. A layer 4 (L4) LoadBalancer service exposes the ALB so that the ALB can receive external requests that come into your cluster. An Ingress deployment consists of three components: Ingress resources, an internal L7 Application Load Balancer (ALB), an external L4 load balancer to handle incoming requests across zones. For classic clusters, this component is the Multi-Zone Load Balancer (MZLB) that IBM Cloud Kubernetes Service creates for you. For VPC clusters, this component is the VPC load balancer created in your VPC. To expose an app by using Ingress, you must create a Kubernetes service for your app and register this service with Ingress by defining an Ingress resource. (To learn how, go to Ingress and ALB ). The following diagram shows how Ingress directs communication from the internet to an app in a VPC multizone cluster. source A VPC load balancer listens for external traffic, and based on the resolved IP address, the VPC load balancer sends the request to an available Application Load Balancer (ALB). The Application Load Balancer (ALB) listens for incoming HTTP, HTTPS, or TCP service requests, checks if routing rules exist for the application, and then forwards requests to the appropriate app pod according to the rules defined in the Ingress resource. In the IKS instance, there is a private and a public Ingress ALB installed, the private Ingress ALB is disabled. ibmcloud ks ingress alb ls -c $MY_CLUSTER_NAME OK ALB ID Enabled State Type Load Balancer Hostname Zone Build Status private-crbvhau5gd0do97g1vvo50-alb1 false disabled private - us-south-1 ingress:/ingress-auth: - public-crbvhau5gd0do97g1vvo50-alb1 true enabled public 1e7d00e2-us-south.lb.appdomain.cloud us-south-1 ingress:0.35.0_826_iks/ingress-auth: - List the VPC Load Balancers that were created, ibmcloud is load-balancers Listing load balancers for generation 2 compute in all resource groups and region us-south under account Remko de Knikker as user b.newell2@remkoh.dev... ID Name Family Subnets Is public Provision status Operating status Resource group r006-1e7d00e2-cc50-4f5b-a12f-b5bcfa95c439 kube-bvhau5gd0do97g1vvo50-c974a95bc72740fba2839a191ba23e17 Application remkohdev-vpcsubnet1 true active online Default r006-7a6a66a7-9871-473e-b105-20640caa0f77 kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481 Application remkohdev-vpcsubnet1 true active online Default The Family property is listed as value Application (Application Load Balancer (ALB)) and the Name is listed as value dynamic . This means that IBM Cloud Application Load Balancer for VPC integrates with instance groups, which can auto scale your back-end members. Pool members are dynamically added and deleted based on your usage and requirements. Round-robin is the default load-balancing method, but you can also use weighted round-robin or least connections. Inspect details for each VPC ALB and inspect Listeners, Pools and Pool Members, MY_LOAD_BALANCER_ID=r006-7a6a66a7-9871-473e-b105-20640caa0f77 ibmcloud is load-balancer $MY_LOAD_BALANCER_ID --output json { \"created_at\": \"2020-12-23T03:29:11.831Z\", \"crn\": \"crn:v1:bluemix:public:is:us-south:a/31296e3a285f42fdadd51ce14beba65e::load-balancer:r006-7a6a66a7-9871-473e-b105-20640caa0f77\", \"hostname\": \"7a6a66a7-us-south.lb.appdomain.cloud\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77\", \"id\": \"r006-7a6a66a7-9871-473e-b105-20640caa0f77\", \"is_public\": true, \"listeners\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\", \"id\": \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" } ], \"logging\": { \"datapath\": { \"active\": false } }, \"name\": \"kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481\", \"operating_status\": \"online\", \"pools\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"id\": \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"name\": \"tcp-3000-32308\" } ], \"private_ips\": [ { \"address\": \"10.240.0.7\" }, { \"address\": \"10.240.0.8\" } ], \"profile\": { \"family\": \"Application\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancer/profiles/dynamic\", \"name\": \"dynamic\" }, \"provisioning_status\": \"active\", \"public_ips\": [ { \"address\": \"52.116.134.164\" }, { \"address\": \"52.116.142.235\" } ], \"resource_group\": { \"href\": \"https://resource-controller.cloud.ibm.com/v1/resource_groups/68af6383f717459686457a6434c4d19f\", \"id\": \"68af6383f717459686457a6434c4d19f\", \"name\": \"Default\" }, \"security_groups\": null, \"subnets\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/subnets/0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\", \"id\": \"0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\", \"name\": \"remkohdev-vpcsubnet1\" } ] } MYLOAD_BALANCER_LISTENER_ID=r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c ibmcloud is load-balancer-listener $MY_LOAD_BALANCER_ID $MYLOAD_BALANCER_LISTENER_ID --output json { \"created_at\": \"2020-12-23T03:29:12.078Z\", \"default_pool\": { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"id\": \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"name\": \"tcp-3000-32308\" }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\", \"id\": \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\", \"policies\": null, \"port\": 3000, \"protocol\": \"tcp\", \"provisioning_status\": \"active\" } ibmcloud is load-balancer-pools $MY_LOAD_BALANCER_ID --output json [ { \"algorithm\": \"round_robin\", \"created_at\": \"2020-12-23T03:29:11.957Z\", \"health_monitor\": { \"delay\": 5, \"max_retries\": 2, \"port\": 32308, \"timeout\": 2, \"type\": \"tcp\" }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"id\": \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\", \"members\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"id\": \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"port\": null, \"target\": null } ], \"name\": \"tcp-3000-32308\", \"protocol\": \"tcp\", \"provisioning_status\": \"active\", \"proxy_protocol\": \"disabled\" } ] MY_LOAD_BALANCER_POOL_ID=r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492 ibmcloud is load-balancer-pool-members $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID --output json [ { \"created_at\": \"2020-12-23T03:29:11.981Z\", \"health\": \"ok\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"id\": \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"port\": 32308, \"provisioning_status\": \"active\", \"target\": { \"address\": \"10.240.0.4\" }, \"weight\": 50 } ] MY_LOAD_BALANCER_POOL_MEMBER_ID=r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51 ibmcloud is load-balancer-pool-member $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID $MY_LOAD_BALANCER_POOL_MEMBER_ID --output json { \"created_at\": \"2020-12-23T03:29:11.981Z\", \"health\": \"ok\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"id\": \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\", \"port\": 32308, \"provisioning_status\": \"active\", \"target\": { \"address\": \"10.240.0.4\" }, \"weight\": 50 } Note: above, I manually copy-pasted the IDs of the VPC resources into the next command, instead of using the full jq syntax each time to retrieve the values and set corresponding environment variables. You see that the second load balancer has a single front-end listener on port 3000 forwarding to a load balancer pool member on port 32308 on IP 10.240.0.4, which is the Private IP of the single worker node of my cluster. kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.240.0.4 Ready <none> 14h v1.18.13+IKS 10.240.0.4 10.240.0.4 Ubuntu 18.04.5 LTS 4.15.0-128-generic containerd://1.3.9 From the Kubernetes docs about External Load Balancer Providers you see that: \"When the Service type is set to LoadBalancer, Kubernetes provides functionality equivalent to type equals ClusterIP to pods within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the Kubernetes pods. The Kubernetes service controller automates the creation of the external load balancer, health checks (if needed), firewall rules (if needed) and retrieves the external IP allocated by the cloud provider and populates it in the service object.\"","title":"Ingress Application Load Balancer\u00a0(ALB)"},{"location":"vpcgen2/#cleanup","text":"Delete your Kubernetes cluster, ibmcloud ks cluster rm --cluster $MY_CLUSTER_NAME Do you want to delete the persistent storage for this cluster? If yes, the data cannot be recovered. If no, you can delete the persistent storage later in your IBM Cloud infrastructure account. [y/N]> y After you run this command, the cluster cannot be restored. Remove the cluster remkohdev-iks118-vpc-cluster1? [y/N]> y Removing cluster remkohdev-iks118-vpc-cluster1, persistent storage... OK Delete the Gateways, Load Balancers, Network Interfaces, subnet, public gateways, and finally delete the VPC, ibmcloud is vpn-gateways ibmcloud is vpn-gateway-delete $vpnid ibmcloud is load-balancers ibmcloud is load-balancer-delete $lbid ibmcloud is instance-network-interfaces $vsi ibmcloud is instances ibmcloud is instance-delete $vsi ibmcloud is subnets ibmcloud is subnet-delete $subnet ibmcloud is public-gateways ibmcloud is public-gateway-delete $gateway ibmcloud is vpcs ibmcloud is vpc-delete $vpc","title":"Cleanup"},{"location":"vpcgen2/#conclusion","text":"You're awesome! Security is an important part of any software application development and \" airgapping \" your cluster by adding a VPC Generation 2 is a first step in securing your cluster, network and containers.","title":"Conclusion"},{"location":"archive/hello-java-app/","text":"How the HelloWorld App was Created \u00b6 Do not The following step is only explaining how the HelloWorld application was created. The Kubernetes resources will actually pull an existing image from Docker Hub. Create the Spring Boot scaffolding, $ spring init --dependencies=web,data-rest,thymeleaf helloworld-api $ cd helloworld-api $ mvn clean install $ mvn test $ mvn spring-boot:run Create the APIController, $ echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.http.HttpStatus; @RestController public class APIController { @Autowired private MessageRepository repository; @GetMapping(\"/api\") public String index() { return \"Welcome to Spring Boot App\"; } @RequestMapping(value = \"/api/hello\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String hello(@RequestParam String name) { String message = \"Hello \"+ name; String responseJson = \"{ \\\"message\\\" : \\\"\"+ message + \"\\\" }\"; repository.save(new Message(name, message)); for (Message msg : repository.findAll()) { System.out.println(msg); } return responseJson; } @RequestMapping(value = \"/api/messages\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public ResponseEntity<List<Message>> getMessages() { List<Message> messages = (List<Message>) repository.findAll(); return new ResponseEntity<List<Message>>(messages, HttpStatus.OK); } } ' > src/main/java/com/example/helloworldapi/APIController.java Create the Message class, $ echo 'package com.example.helloworldapi; import org.springframework.data.annotation.Id; public class Message { @Id private String id; private String sender; private String message; public Message(String sender, String message) { this.sender = sender; this.message = message; } public String getId() { return id; } public String getSender() { return sender; } public String getMessage() { return message; } @Override public String toString() { return \"Message [id=\" + id + \", sender=\" + sender + \", message=\" + message + \"]\"; } }' > src/main/java/com/example/helloworldapi/Message.java Create the MessageRepository class, echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.data.mongodb.repository.MongoRepository; public interface MessageRepository extends MongoRepository<Message, String> { public List<Message> findBySender(String sender); }' > src/main/java/com/example/helloworldapi/MessageRepository.java Add a new file \u2018~/src/main/resources/application.properties\u2019, whose configuration should match those when the MongoDB server was deployed, echo 'pring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=mongodb.default' > src/main/resources/application.properties Add the following dependency to the Maven build file in pom.xml , <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-mongodb</artifactId> </dependency> </dependencies> Test Java App on localhost: \u00b6 Create local Mongodb, docker run --name mongo -d -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=passw0rd mongo Get IP address on mac osx, $ ifconfig en0 | grep inet 192.168.86.45 Configure application.properties and change the host to your local IP Address to test the app on localhost. On the localhost we did not create a separate user, but you can use the admin user. spring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=192.168.86.45 Run the application $ mvn clean install $ mvn spring-boot:run test $ curl -X GET 'http://192.168.86.45:8080/api/hello?name=one' { \"message\" : \"Hello one\" } Dockerize \u00b6 echo 'FROM openjdk:8-jdk-alpine ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} app.jar EXPOSE 8080 ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]' > Dockerfile Clean, install, build, login to Docker, tag, push, mvn clean install docker image build -t helloworld . docker run -d --name helloworld -p 8081:8080 helloworld docker login -u <username> docker tag helloworld remkohdev/helloworld:lab1v1.0 docker push remkohdev/helloworld:lab1v1.0","title":"Hello java app"},{"location":"archive/hello-java-app/#how-the-helloworld-app-was-created","text":"Do not The following step is only explaining how the HelloWorld application was created. The Kubernetes resources will actually pull an existing image from Docker Hub. Create the Spring Boot scaffolding, $ spring init --dependencies=web,data-rest,thymeleaf helloworld-api $ cd helloworld-api $ mvn clean install $ mvn test $ mvn spring-boot:run Create the APIController, $ echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.http.HttpStatus; @RestController public class APIController { @Autowired private MessageRepository repository; @GetMapping(\"/api\") public String index() { return \"Welcome to Spring Boot App\"; } @RequestMapping(value = \"/api/hello\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String hello(@RequestParam String name) { String message = \"Hello \"+ name; String responseJson = \"{ \\\"message\\\" : \\\"\"+ message + \"\\\" }\"; repository.save(new Message(name, message)); for (Message msg : repository.findAll()) { System.out.println(msg); } return responseJson; } @RequestMapping(value = \"/api/messages\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public ResponseEntity<List<Message>> getMessages() { List<Message> messages = (List<Message>) repository.findAll(); return new ResponseEntity<List<Message>>(messages, HttpStatus.OK); } } ' > src/main/java/com/example/helloworldapi/APIController.java Create the Message class, $ echo 'package com.example.helloworldapi; import org.springframework.data.annotation.Id; public class Message { @Id private String id; private String sender; private String message; public Message(String sender, String message) { this.sender = sender; this.message = message; } public String getId() { return id; } public String getSender() { return sender; } public String getMessage() { return message; } @Override public String toString() { return \"Message [id=\" + id + \", sender=\" + sender + \", message=\" + message + \"]\"; } }' > src/main/java/com/example/helloworldapi/Message.java Create the MessageRepository class, echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.data.mongodb.repository.MongoRepository; public interface MessageRepository extends MongoRepository<Message, String> { public List<Message> findBySender(String sender); }' > src/main/java/com/example/helloworldapi/MessageRepository.java Add a new file \u2018~/src/main/resources/application.properties\u2019, whose configuration should match those when the MongoDB server was deployed, echo 'pring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=mongodb.default' > src/main/resources/application.properties Add the following dependency to the Maven build file in pom.xml , <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-mongodb</artifactId> </dependency> </dependencies>","title":"How the HelloWorld App was Created"},{"location":"archive/hello-java-app/#test-java-app-on-localhost","text":"Create local Mongodb, docker run --name mongo -d -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=passw0rd mongo Get IP address on mac osx, $ ifconfig en0 | grep inet 192.168.86.45 Configure application.properties and change the host to your local IP Address to test the app on localhost. On the localhost we did not create a separate user, but you can use the admin user. spring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=192.168.86.45 Run the application $ mvn clean install $ mvn spring-boot:run test $ curl -X GET 'http://192.168.86.45:8080/api/hello?name=one' { \"message\" : \"Hello one\" }","title":"Test Java App on localhost:"},{"location":"archive/hello-java-app/#dockerize","text":"echo 'FROM openjdk:8-jdk-alpine ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} app.jar EXPOSE 8080 ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]' > Dockerfile Clean, install, build, login to Docker, tag, push, mvn clean install docker image build -t helloworld . docker run -d --name helloworld -p 8081:8080 helloworld docker login -u <username> docker tag helloworld remkohdev/helloworld:lab1v1.0 docker push remkohdev/helloworld:lab1v1.0","title":"Dockerize"},{"location":"archive/hello-proxy-setup/","text":"HelloWorld App Proxy Setup \u00b6 I want to deploy two versions of helloworld , a direct helloworld and a helloworld-proxy , which is a proxy to the helloworld application. To access the direct version, I add a path /helloworld . To access the proxy version, I add a path /helloworld/proxy . First deploy the proxy. $ kubectl create -f helloworld-proxy-deployment.yaml $ kubectl create -f helloworld-proxy-service-loadbalancer.yaml Get the proxy service details and test the proxy, $ kubectl get svc helloworld-proxy NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld-proxy LoadBalancer 172.21.78.158 169.48.67.164 8080:30940/TCP 61s To test use the External-IP and the NodePort, e.g. 169.48.67.164:30940, and send the proxy the Kubernetes service name and target port, $ $ curl -L -X POST \"http://169.48.67.164:30940/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld-proxy:8080\" }' {\"id\":\"3aa4f889-94a0-4be8-b8f2-8b59f8ad3de7\",\"sender\":\"remko\",\"message\":\"Hello remko (proxy)\",\"host\":\"helloworld-proxy:8080\"} The proxy will add a proxy message to the created message. You need the Ingress Subdomain and Ingress Secret of your cluster to configure your Ingress resource. Resources \u00b6 See: https://kubernetes.io/docs/concepts/services-networking/ingress/ https://medium.com/google-cloud/understanding-kubernetes-networking-ingress-1bc341c84078 https://github.com/ibm-cloud-docs/containers/blob/master/cs_ingress.md Kong, \"the world's most popular open source API Gateway\" , tutorial with Kong Ingress","title":"HelloWorld App Proxy Setup"},{"location":"archive/hello-proxy-setup/#helloworld-app-proxy-setup","text":"I want to deploy two versions of helloworld , a direct helloworld and a helloworld-proxy , which is a proxy to the helloworld application. To access the direct version, I add a path /helloworld . To access the proxy version, I add a path /helloworld/proxy . First deploy the proxy. $ kubectl create -f helloworld-proxy-deployment.yaml $ kubectl create -f helloworld-proxy-service-loadbalancer.yaml Get the proxy service details and test the proxy, $ kubectl get svc helloworld-proxy NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld-proxy LoadBalancer 172.21.78.158 169.48.67.164 8080:30940/TCP 61s To test use the External-IP and the NodePort, e.g. 169.48.67.164:30940, and send the proxy the Kubernetes service name and target port, $ $ curl -L -X POST \"http://169.48.67.164:30940/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld-proxy:8080\" }' {\"id\":\"3aa4f889-94a0-4be8-b8f2-8b59f8ad3de7\",\"sender\":\"remko\",\"message\":\"Hello remko (proxy)\",\"host\":\"helloworld-proxy:8080\"} The proxy will add a proxy message to the created message. You need the Ingress Subdomain and Ingress Secret of your cluster to configure your Ingress resource.","title":"HelloWorld App Proxy Setup"},{"location":"archive/hello-proxy-setup/#resources","text":"See: https://kubernetes.io/docs/concepts/services-networking/ingress/ https://medium.com/google-cloud/understanding-kubernetes-networking-ingress-1bc341c84078 https://github.com/ibm-cloud-docs/containers/blob/master/cs_ingress.md Kong, \"the world's most popular open source API Gateway\" , tutorial with Kong Ingress","title":"Resources"},{"location":"archive/ingress-byo/","text":"Bring your own Ingress controller \u00b6 run Ingress on a free 1 node IKS cluster In IBM Cloud Kubernetes Service, IBM-provided application load balancers (ALBs) are based on a custom implementation of the NGINX Ingress controller. However, depending on what your app requires, you might want to configure your own custom Ingress controller instead of using the IBM-provided ALBs. For example, you might want to use the Istio ingressgateway load balancer service to control traffic for your cluster. When you bring your own Ingress controller, you are responsible for supplying the controller image, maintaining the controller, updating the controller, and any security-related updates to keep your Ingress controller free from vulnerabilities. Expose your Ingress controller by creating an NLB and a hostname \u00b6 Create a free cluster Login to your cluster List existing NLBs CLUSTERNAME=remkohdev-iks116-1n-cluster-free % ibmcloud ks nlb-dns ls -c $CLUSTERNAME OK Subdomain Load Balancer Hostname SSL Cert Status SSL Cert Secret Name Secret Namespace kubectl config current-context ibmcloud ks worker ls --cluster $CLUSTERNAME OK ID Public IP Private IP Flavor State Status Zone Version kube-br2lrckd0icvsu163bt0-remkohdevik-default-000000f5 173.193.106.26 10.76.114.251 free normal Ready hou02 1.16.9_1531 % ibmcloud ks zones --provider classic % ibmcloud ks vlan ls --zone OK ID Name Number Type Router Supports Virtual Workers ibmcloud ks cluster get --cluster $CLUSTERNAME % ibmcloud ks worker-pool ls --cluster $CLUSTERNAME OK Name ID Flavor Workers default br1vf5df00av3tu5nbd0-45ebec5 free 1 Create a Network Load Balancer (NLB) to expose your custom Ingress controller deployment by creating a LoadBalancer service, echo 'apiVersion: v1 kind: Service metadata: name: my-ingress labels: app: my-ingress spec: ports: - port: 80 selector: app: my-ingress type: LoadBalancer' > my-ingress.yaml kubectl create -f my-ingress.yaml apiVersion: v1 kind: Service metadata: name: my-ingress annotations: service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: service.kubernetes.io/ibm-load-balancer-cloud-provider-vlan: \" \" spec: type: LoadBalancer selector: : ports: - protocol: TCP port: 8080 loadBalancerIP: and then create a hostname for the NLB IP address. Get the configuration file for your Ingress controller ready. For example, you can use the cloud-generic NGINX community Ingress controller. If you use the community controller, edit the mandatory.yaml file by following these steps. Replace all instances of namespace: ingress-nginx with namespace: kube-system. Replace all instances of the app.kubernetes.io/name: ingress-nginx and app.kubernetes.io/part-of: ingress-nginx labels with one app: ingress-nginx label. Deploy your own Ingress controller. For example, to use the cloud-generic NGINX community Ingress controller, run the following command. kubectl apply -f mandatory.yaml -n kube-system Define a load balancer service to expose your custom Ingress deployment. apiVersion: v1 kind: Service metadata: name: ingress-nginx spec: type: LoadBalancer selector: app: ingress-nginx ports: - name: http protocol: TCP port: 80 - name: https protocol: TCP port: 443 externalTrafficPolicy: Cluster Create the service in your cluster. kubectl apply -f ingress-nginx.yaml -n kube-system Get the EXTERNAL-IP address for the load balancer. kubectl get svc ingress-nginx -n kube-system","title":"Bring your own Ingress controller"},{"location":"archive/ingress-byo/#bring-your-own-ingress-controller","text":"run Ingress on a free 1 node IKS cluster In IBM Cloud Kubernetes Service, IBM-provided application load balancers (ALBs) are based on a custom implementation of the NGINX Ingress controller. However, depending on what your app requires, you might want to configure your own custom Ingress controller instead of using the IBM-provided ALBs. For example, you might want to use the Istio ingressgateway load balancer service to control traffic for your cluster. When you bring your own Ingress controller, you are responsible for supplying the controller image, maintaining the controller, updating the controller, and any security-related updates to keep your Ingress controller free from vulnerabilities.","title":"Bring your own Ingress controller"},{"location":"archive/ingress-byo/#expose-your-ingress-controller-by-creating-an-nlb-and-a-hostname","text":"Create a free cluster Login to your cluster List existing NLBs CLUSTERNAME=remkohdev-iks116-1n-cluster-free % ibmcloud ks nlb-dns ls -c $CLUSTERNAME OK Subdomain Load Balancer Hostname SSL Cert Status SSL Cert Secret Name Secret Namespace kubectl config current-context ibmcloud ks worker ls --cluster $CLUSTERNAME OK ID Public IP Private IP Flavor State Status Zone Version kube-br2lrckd0icvsu163bt0-remkohdevik-default-000000f5 173.193.106.26 10.76.114.251 free normal Ready hou02 1.16.9_1531 % ibmcloud ks zones --provider classic % ibmcloud ks vlan ls --zone OK ID Name Number Type Router Supports Virtual Workers ibmcloud ks cluster get --cluster $CLUSTERNAME % ibmcloud ks worker-pool ls --cluster $CLUSTERNAME OK Name ID Flavor Workers default br1vf5df00av3tu5nbd0-45ebec5 free 1 Create a Network Load Balancer (NLB) to expose your custom Ingress controller deployment by creating a LoadBalancer service, echo 'apiVersion: v1 kind: Service metadata: name: my-ingress labels: app: my-ingress spec: ports: - port: 80 selector: app: my-ingress type: LoadBalancer' > my-ingress.yaml kubectl create -f my-ingress.yaml apiVersion: v1 kind: Service metadata: name: my-ingress annotations: service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: service.kubernetes.io/ibm-load-balancer-cloud-provider-vlan: \" \" spec: type: LoadBalancer selector: : ports: - protocol: TCP port: 8080 loadBalancerIP: and then create a hostname for the NLB IP address. Get the configuration file for your Ingress controller ready. For example, you can use the cloud-generic NGINX community Ingress controller. If you use the community controller, edit the mandatory.yaml file by following these steps. Replace all instances of namespace: ingress-nginx with namespace: kube-system. Replace all instances of the app.kubernetes.io/name: ingress-nginx and app.kubernetes.io/part-of: ingress-nginx labels with one app: ingress-nginx label. Deploy your own Ingress controller. For example, to use the cloud-generic NGINX community Ingress controller, run the following command. kubectl apply -f mandatory.yaml -n kube-system Define a load balancer service to expose your custom Ingress deployment. apiVersion: v1 kind: Service metadata: name: ingress-nginx spec: type: LoadBalancer selector: app: ingress-nginx ports: - name: http protocol: TCP port: 80 - name: https protocol: TCP port: 443 externalTrafficPolicy: Cluster Create the service in your cluster. kubectl apply -f ingress-nginx.yaml -n kube-system Get the EXTERNAL-IP address for the load balancer. kubectl get svc ingress-nginx -n kube-system","title":"Expose your Ingress controller by creating an NLB and a hostname"},{"location":"archive/networkpolicy-notes/","text":"find your cloud shell name $ ls -al /tmp/ic .. cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 $ SHELL=cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 Find the cluster's admin folder $ ls -al /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/ .. remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0 remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin $ ADMINDIR=remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/$ADMINDIR/calicoctl.cfg $ mv /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/$ADMINDIR/calicoctl.cfg /etc/calico $ calicoctl get nodes NAME kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 $ calicoctl version Client Version: v3.10.0 Git commit: 7968b525 Cluster Version: v3.9.5 Cluster Type: k8s,bgp View network policy $ calicoctl get hostendpoint -o yaml apiVersion: projectcalico.org/v3 items: - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:51Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-private resourceVersion: \"2643\" uid: e0eaf0d6-a36b-11ea-8f29-06ce448b0c2d spec: expectedIPs: - 10.187.222.149 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:39Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-public resourceVersion: \"2642\" uid: 78ec7195-2ddd-4b5f-b7c4-8a54d25f4beb spec: expectedIPs: - 150.238.93.101 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:01Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-private resourceVersion: \"2295\" uid: c2fcd831-a36b-11ea-97f2-06edddc7e672 spec: expectedIPs: - 10.187.222.146 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:22:54Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-public resourceVersion: \"2294\" uid: e8c1fb2b-1a88-4cb4-a10e-67c9bdd28e1c spec: expectedIPs: - 150.238.93.100 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 kind: HostEndpointList metadata: resourceVersion: \"40605\" $ calicoctl get NetworkPolicy --all-namespaces -o wide NAMESPACE NAME ORDER SELECTOR kube-system knp.default.kubernetes-dashboard 1000 projectcalico.org/orchestrator == 'k8s' && k8s-app == 'kubernetes-dashboard' $ calicoctl get GlobalNetworkPolicy -o wide NAME ORDER SELECTOR allow-all-outbound 1900 ibm.role in { 'worker_public', 'master_public' } allow-all-private-default 100000 ibm.role == 'worker_private' allow-bigfix-port 1900 ibm.role in { 'worker_public', 'master_public' } allow-icmp 1500 ibm.role in { 'worker_public', 'master_public' } allow-node-port-dnat 1500 ibm.role == 'worker_public' allow-sys-mgmt 1950 ibm.role in { 'worker_public', 'master_public' } allow-vrrp 1500 ibm.role == 'worker_public' $ echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: access-nginx namespace: advanced-policy-demo spec: podSelector: matchLabels: app: helloworld ingress: - from: - podSelector: matchLabels: {}' > helloworld-policy-allow-to-helloworld.yaml","title":"Networkpolicy notes"},{"location":"references/contributors/","text":"Contributors \u00b6 Remko de Knikker, remkohdev , Tim Robinson, timroster ,","title":"Contributors"},{"location":"references/contributors/#contributors","text":"Remko de Knikker, remkohdev , Tim Robinson, timroster ,","title":"Contributors"}]}